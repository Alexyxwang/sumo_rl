{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dqn_hyperparam_tuning.ipynb\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import traci\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SUMO Configuration ---\n",
    "def sumo_config(traffic_pattern=\"P1\"):\n",
    "    sumo_config = [\n",
    "        \"sumo\",\n",
    "        \"-c\", f\"SUMO_networks/{traffic_pattern}/junction.sumocfg\",\n",
    "        \"--step-length\", \"0.05\",\n",
    "        \"--delay\", \"0\",\n",
    "        \"--lateral-resolution\", \"0.1\",\n",
    "        \"--start\",\n",
    "        \"--no-warnings\",\n",
    "        \"--no-step-log\",\n",
    "    ]\n",
    "    return sumo_config\n",
    "\n",
    "if \"SUMO_HOME\" in os.environ:\n",
    "    tools = os.path.join(os.environ[\"SUMO_HOME\"], \"tools\")\n",
    "    sys.path.append(tools)\n",
    "else:\n",
    "    sys.exit(\"Please declare environment variable 'SUMO_HOME'\")\n",
    "\n",
    "if not traci.isLoaded():\n",
    "    traci.start(sumo_config())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def save_episode_plots(metrics, combo_name=\"default\"):\n",
    "    os.makedirs(f\"Plots/{combo_name}\", exist_ok=True)\n",
    "\n",
    "    # Plot total reward per episode\n",
    "    plt.figure()\n",
    "    plt.plot(metrics[\"rewards_per_episode\"])\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Total Reward\")\n",
    "    plt.title(f\"{combo_name} - Reward per Episode\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"Plots/{combo_name}/rewards_per_episode.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Plot average wait per episode\n",
    "    plt.figure()\n",
    "    plt.plot(metrics[\"avg_wait_per_ep\"])\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Average Wait Time\")\n",
    "    plt.title(f\"{combo_name} - Avg Wait per Episode\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"Plots/{combo_name}/avg_wait_per_episode.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Plot maximum wait per episode\n",
    "    plt.figure()\n",
    "    plt.plot(metrics[\"max_wait_per_ep\"])\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Maximum Wait Time\")\n",
    "    plt.title(f\"{combo_name} - Max Wait per Episode\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"Plots/{combo_name}/max_wait_per_episode.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Per-lane average queue length plots\n",
    "    avg_queues = metrics[\"avg_queue_lengths\"]\n",
    "    for lane_index in range(avg_queues.shape[0]):\n",
    "        plt.figure()\n",
    "        plt.plot(avg_queues[lane_index].numpy())\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Average Queue Length\")\n",
    "        plt.title(f\"{combo_name} - Lane {lane_index} Avg Queue Length\")\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"Plots/{combo_name}/avg_queue_lane_{lane_index}.png\")\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Global Variables ---\n",
    "action_space_size = 8\n",
    "TRAFFIC_LIGHT_ID = \"traffic_light\"\n",
    "DELTA_PHASE_DURATION = 6\n",
    "YELLOW_PHASE_DURATION = 4\n",
    "lane_detectors = [f'q{i+1}' for i in range(8)]\n",
    "current_phase = 2\n",
    "\n",
    "def get_queue_length():\n",
    "    return torch.tensor([\n",
    "        traci.lanearea.getLastStepHaltingNumber(d) for d in lane_detectors\n",
    "    ], dtype=torch.float)\n",
    "\n",
    "def get_current_state():\n",
    "    return get_queue_length()\n",
    "\n",
    "def simulate_time(seconds=1):\n",
    "    for _ in range(20 * seconds):\n",
    "        traci.simulationStep()\n",
    "\n",
    "# def step(action):\n",
    "#     global current_phase\n",
    "#     if 2 * action == current_phase:\n",
    "#         traci.trafficlight.setPhase(TRAFFIC_LIGHT_ID, 2 * action)\n",
    "#         simulate_time(DELTA_PHASE_DURATION)\n",
    "#     else:\n",
    "#         traci.trafficlight.setPhase(TRAFFIC_LIGHT_ID, current_phase + 1)\n",
    "#         simulate_time(YELLOW_PHASE_DURATION)\n",
    "#         current_phase = 2 * action\n",
    "#         traci.trafficlight.setPhase(TRAFFIC_LIGHT_ID, 2 * action)\n",
    "#         simulate_time(DELTA_PHASE_DURATION)\n",
    "#     next_state = get_current_state()\n",
    "#     reward = -torch.sum(next_state)\n",
    "#     done = traci.simulation.getMinExpectedNumber() == 0\n",
    "#     return next_state, reward, done\n",
    "\n",
    "def step(action):\n",
    "    global current_phase\n",
    "    if 2 * action == current_phase:\n",
    "        traci.trafficlight.setPhase(TRAFFIC_LIGHT_ID, 2 * action)\n",
    "        simulate_time(DELTA_PHASE_DURATION)\n",
    "    else:\n",
    "        traci.trafficlight.setPhase(TRAFFIC_LIGHT_ID, current_phase + 1)\n",
    "        simulate_time(YELLOW_PHASE_DURATION)\n",
    "        current_phase = 2 * action\n",
    "        traci.trafficlight.setPhase(TRAFFIC_LIGHT_ID, 2 * action)\n",
    "        simulate_time(DELTA_PHASE_DURATION)\n",
    "        \n",
    "    next_state = get_current_state()\n",
    "    reward = -torch.sum(next_state)\n",
    "    done = traci.simulation.getMinExpectedNumber() == 0\n",
    "    return next_state, reward, done, next_state.clone()  # added queue tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Neural Network ---\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 64), nn.ReLU(),\n",
    "            nn.Linear(64, action_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Environment Switching ---\n",
    "import itertools\n",
    "traffic_patterns = itertools.cycle([\"P1\", \"P2\", \"P3\", \"P4\"])\n",
    "\n",
    "def change_env():\n",
    "    pattern = next(traffic_patterns)\n",
    "    if traci.isLoaded():\n",
    "        traci.close()\n",
    "    traci.start(sumo_config(pattern))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Choose Action ---\n",
    "def choose_action(state, epsilon, policy_net):\n",
    "    if random.random() < epsilon:\n",
    "        return random.randint(0, action_space_size - 1)\n",
    "    else:\n",
    "        return torch.argmax(policy_net(state.unsqueeze(0))).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Optimizer Step ---\n",
    "def optimise_model(policy_net, target_net, memory, optimizer, batch_size, gamma):\n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "    batch = random.sample(memory, batch_size)\n",
    "    states = torch.stack([x[0] for x in batch])\n",
    "    actions = torch.tensor([x[1] for x in batch]).unsqueeze(1)\n",
    "    rewards = torch.tensor([x[2] for x in batch], dtype=torch.float)\n",
    "    next_states = torch.stack([x[3] for x in batch])\n",
    "    dones = torch.tensor([x[4] for x in batch], dtype=torch.float)\n",
    "\n",
    "    q_vals = policy_net(states).gather(1, actions).squeeze()\n",
    "    with torch.no_grad():\n",
    "        max_next_q_vals = target_net(next_states).max(1)[0]\n",
    "        target_vals = rewards + gamma * max_next_q_vals * (1 - dones)\n",
    "    loss = nn.MSELoss()(q_vals, target_vals)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Training Loop ---\n",
    "# def train_algorithm(params, episodes=10):\n",
    "#     gamma = params[\"gamma\"]\n",
    "#     epsilon = params[\"epsilon\"]\n",
    "#     epsilon_decay = params[\"epsilon_decay\"]\n",
    "#     min_epsilon = params[\"min_epsilon\"]\n",
    "#     lr = params[\"learning_rate\"]\n",
    "#     batch_size = params[\"batch_size\"]\n",
    "#     target_update_freq = params[\"target_update_freq\"]\n",
    "#     memory_size = params[\"memory_size\"]\n",
    "\n",
    "#     state_dim = len(lane_detectors)\n",
    "#     policy_net = DQN(state_dim, action_space_size)\n",
    "#     target_net = DQN(state_dim, action_space_size)\n",
    "#     target_net.load_state_dict(policy_net.state_dict())\n",
    "#     optimizer = optim.Adam(policy_net.parameters(), lr=lr)\n",
    "#     memory = deque(maxlen=memory_size)\n",
    "\n",
    "#     rewards_per_episode = []\n",
    "#     steps_done = 0\n",
    "\n",
    "#     for episode in range(episodes):\n",
    "#         change_env()\n",
    "#         state = get_current_state()\n",
    "#         episode_reward = 0\n",
    "#         done = False\n",
    "\n",
    "#         while not done:\n",
    "#             action = choose_action(state, epsilon, policy_net)\n",
    "#             next_state, reward, done = step(action)\n",
    "#             memory.append((state, action, reward, next_state, done))\n",
    "#             state = next_state\n",
    "#             episode_reward += reward\n",
    "#             optimise_model(policy_net, target_net, memory, optimizer, batch_size, gamma)\n",
    "#             if steps_done % target_update_freq == 0:\n",
    "#                 target_net.load_state_dict(policy_net.state_dict())\n",
    "#             steps_done += 1\n",
    "\n",
    "#         epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "#         rewards_per_episode.append(episode_reward.item())\n",
    "\n",
    "#     return sum(rewards_per_episode) / episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_algorithm(params, episodes=10):\n",
    "    gamma = params[\"gamma\"]\n",
    "    epsilon = params[\"epsilon\"]\n",
    "    epsilon_decay = params[\"epsilon_decay\"]\n",
    "    min_epsilon = params[\"min_epsilon\"]\n",
    "    lr = params[\"learning_rate\"]\n",
    "    batch_size = params[\"batch_size\"]\n",
    "    target_update_freq = params[\"target_update_freq\"]\n",
    "    memory_size = params[\"memory_size\"]\n",
    "\n",
    "    state_dim = len(lane_detectors)\n",
    "    policy_net = DQN(state_dim, action_space_size)\n",
    "    target_net = DQN(state_dim, action_space_size)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=lr)\n",
    "    memory = deque(maxlen=memory_size)\n",
    "\n",
    "    rewards_per_episode = []\n",
    "    steps_done = 0\n",
    "\n",
    "    avg_wait_per_ep = []\n",
    "    max_wait_per_ep = []\n",
    "    all_avg_queue_lengths = torch.zeros(len(lane_detectors), episodes)\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        change_env()\n",
    "        state = get_current_state()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "\n",
    "        vehicle_wait_tracker = {}\n",
    "        queue_length_tracker = {}\n",
    "        num_steps = 0\n",
    "\n",
    "        while not done:\n",
    "            action = choose_action(state, epsilon, policy_net)\n",
    "            next_state, reward, done, curr_queue = step(action)\n",
    "            memory.append((state, action, reward, next_state, done))\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "\n",
    "            # Vehicle wait time tracking\n",
    "            for v_id in traci.vehicle.getIDList():\n",
    "                wait_time = traci.vehicle.getWaitingTime(v_id)\n",
    "                if v_id not in vehicle_wait_tracker:\n",
    "                    vehicle_wait_tracker[v_id] = wait_time\n",
    "                elif wait_time > vehicle_wait_tracker[v_id]:\n",
    "                    vehicle_wait_tracker[v_id] = wait_time\n",
    "\n",
    "            # Queue length tracking\n",
    "            for i in range(len(curr_queue)):\n",
    "                queue_length_tracker[i] = queue_length_tracker.get(i, 0) + curr_queue[i]\n",
    "\n",
    "            optimise_model(policy_net, target_net, memory, optimizer, batch_size, gamma)\n",
    "            if steps_done % target_update_freq == 0:\n",
    "                target_net.load_state_dict(policy_net.state_dict())\n",
    "            steps_done += 1\n",
    "            num_steps += 1\n",
    "\n",
    "        # Aggregate queue stats\n",
    "        for i, total_len in queue_length_tracker.items():\n",
    "            all_avg_queue_lengths[i, episode] = total_len / num_steps\n",
    "\n",
    "        vehicle_waits = list(vehicle_wait_tracker.values())\n",
    "        avg_wait = sum(vehicle_waits) / len(vehicle_waits) if vehicle_waits else 0.0\n",
    "        max_wait = max(vehicle_waits) if vehicle_waits else 0.0\n",
    "        avg_wait_per_ep.append(avg_wait)\n",
    "        max_wait_per_ep.append(max_wait)\n",
    "\n",
    "        epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "        rewards_per_episode.append(episode_reward.item())\n",
    "\n",
    "    N = max(1, int(0.2 * episodes))  # 20% of episodes, at least 1\n",
    "    avg_reward_last_N = sum(rewards_per_episode[-N:]) / N\n",
    "    avg_wait_last_N = sum(avg_wait_per_ep[-N:]) / N\n",
    "    max_wait_last_N = max(max_wait_per_ep[-N:]) if max_wait_per_ep[-N:] else 0.0\n",
    "\n",
    "    return {\n",
    "        \"avg_reward\": sum(rewards_per_episode) / episodes,\n",
    "        \"avg_wait_per_ep\": avg_wait_per_ep,\n",
    "        \"max_wait_per_ep\": max_wait_per_ep,\n",
    "        \"avg_queue_lengths\": all_avg_queue_lengths,\n",
    "        \"rewards_per_episode\": rewards_per_episode,\n",
    "        \"avg_reward_last_N\": avg_reward_last_N,\n",
    "        \"avg_wait_last_N\": avg_wait_last_N,\n",
    "        \"max_wait_last_N\": max_wait_last_N,\n",
    "        \"trained_model\": policy_net\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "544ec7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Grid Search ---\n",
    "param_grid = {\n",
    "    \"gamma\": [0.999],  \n",
    "    \"epsilon\": [0.9],          \n",
    "    \"epsilon_decay\": [0.97, 0.99],\n",
    "    \"min_epsilon\": [0.05],    \n",
    "    \"learning_rate\": [0.001],  \n",
    "    \"batch_size\": [128],   \n",
    "    \"target_update_freq\": [1800],\n",
    "    \"memory_size\": [10000, 20000]\n",
    "}\n",
    "\n",
    "keys, values = zip(*param_grid.items())\n",
    "param_combinations = [dict(zip(keys, v)) for v in product(*values)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25756aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = []\n",
    "\n",
    "# for combo in param_combinations:\n",
    "#     print(f\"Running combo: {combo}\")\n",
    "#     avg_reward = train_algorithm(combo, episodes=150)\n",
    "#     combo_result = combo.copy()\n",
    "#     combo_result[\"avg_reward\"] = avg_reward\n",
    "#     results.append(combo_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3ff4fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running combo: {'gamma': 0.999, 'epsilon': 0.9, 'epsilon_decay': 0.97, 'min_epsilon': 0.05, 'learning_rate': 0.001, 'batch_size': 128, 'target_update_freq': 1800, 'memory_size': 10000}\n",
      "Running combo: {'gamma': 0.999, 'epsilon': 0.9, 'epsilon_decay': 0.97, 'min_epsilon': 0.05, 'learning_rate': 0.001, 'batch_size': 128, 'target_update_freq': 1800, 'memory_size': 20000}\n",
      "Running combo: {'gamma': 0.999, 'epsilon': 0.9, 'epsilon_decay': 0.99, 'min_epsilon': 0.05, 'learning_rate': 0.001, 'batch_size': 128, 'target_update_freq': 1800, 'memory_size': 10000}\n",
      "Running combo: {'gamma': 0.999, 'epsilon': 0.9, 'epsilon_decay': 0.99, 'min_epsilon': 0.05, 'learning_rate': 0.001, 'batch_size': 128, 'target_update_freq': 1800, 'memory_size': 20000}\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for i, combo in enumerate(param_combinations):\n",
    "    print(f\"Running combo: {combo}\")\n",
    "    metrics = train_algorithm(combo, episodes=200)\n",
    "\n",
    "    # Save results\n",
    "    combo_result = combo.copy()\n",
    "    combo_result[\"avg_reward\"] = metrics[\"avg_reward\"]\n",
    "    combo_result[\"avg_reward_last_N\"] = metrics[\"avg_reward_last_N\"]\n",
    "    combo_result[\"avg_wait_last_N\"] = metrics[\"avg_wait_last_N\"]\n",
    "    combo_result[\"max_wait_last_N\"] = metrics[\"max_wait_last_N\"]\n",
    "    results.append(combo_result)\n",
    "\n",
    "    combo_name = f\"combo_{i+1}_\" + \"_\".join(f\"{k}-{v}\" for k, v in combo.items())\n",
    "\n",
    "    save_episode_plots(metrics, combo_name=combo_name)\n",
    "\n",
    "    os.makedirs(\"Trained_Models\", exist_ok=True)\n",
    "    model_path = f\"Trained_Models/{combo_name}.pt\"\n",
    "    torch.save(metrics[\"trained_model\"].state_dict(), model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(\"grid_search_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   gamma  epsilon  epsilon_decay  min_epsilon  learning_rate  batch_size  \\\n",
      "0  0.999      0.9           0.97         0.05          0.001         128   \n",
      "1  0.999      0.9           0.97         0.05          0.001         128   \n",
      "3  0.999      0.9           0.99         0.05          0.001         128   \n",
      "2  0.999      0.9           0.99         0.05          0.001         128   \n",
      "\n",
      "   target_update_freq  memory_size  avg_reward  avg_reward_last_N  \\\n",
      "0                1800        10000   -1355.785          -1106.000   \n",
      "1                1800        20000   -1346.825          -1108.300   \n",
      "3                1800        20000   -1915.690          -1228.725   \n",
      "2                1800        10000   -1910.805          -1329.250   \n",
      "\n",
      "   avg_wait_last_N  max_wait_last_N  \n",
      "0         7.018259           298.45  \n",
      "1         7.048160           297.75  \n",
      "3         7.894369           296.95  \n",
      "2         8.555047           298.40  \n"
     ]
    }
   ],
   "source": [
    "print(results_df.sort_values(\"avg_reward_last_N\", ascending=False).head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
