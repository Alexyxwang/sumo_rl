{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5505997f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6343e42",
   "metadata": {},
   "source": [
    "## Problem Statement ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f83c18",
   "metadata": {},
   "source": [
    "Inefficient traffic light control results in increased levels of congestion. Higher levels of congestion will have detrimental impacts on society, as drivers will have more idle time which leads to the following issues:\n",
    "- Poorer quality of life/Driver frustration due to being stuck in traffic\n",
    "- Longer travel time due to delays in traffic\n",
    "- Reduced economic productivity\n",
    "- Higher levels of greenhouse emissions as vehicles will be on the road longer\n",
    "\n",
    "Currently, Sydney adopts mostly fixed timers to control traffic lights. A traffic light control system based on fixed timers has caused delays in traffic such that it takes on average 15 more minutes to reach the same destination. Additionally, there's 6.1 billion in lost productivity annually. This issue is becoming increasingly relevant as Sydney's population is projected tor is and be 6.4 million by 2036. This means there will be more drivers on the road, which will contribute to higher levels of congestion.\n",
    "\n",
    "Our objective is to optimise Sydney's traffic flow and reduce congestion by developing a traffic light control system that is efficient under most scenarios.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931f5378",
   "metadata": {},
   "source": [
    "## Purpose ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f555cc",
   "metadata": {},
   "source": [
    "To address this issue, we considered a reinforcement learning approach. As opposed to other approaches, reinforcement learning is able to handle sequential data through associating actions and delayed rewards. Optimising traffic flow has its applications in other fields, namely factoring queuing systems and warehouse automation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dqn_hyperparam_tuning.ipynb\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import traci\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SUMO Configuration ---\n",
    "def sumo_config(traffic_pattern=\"P1\"):\n",
    "    sumo_config = [\n",
    "        \"sumo\",\n",
    "        \"-c\", f\"SUMO_networks/{traffic_pattern}/junction.sumocfg\",\n",
    "        \"--step-length\", \"0.05\",\n",
    "        \"--delay\", \"0\",\n",
    "        \"--lateral-resolution\", \"0.1\",\n",
    "        \"--start\",\n",
    "        \"--no-warnings\",\n",
    "        \"--no-step-log\",\n",
    "    ]\n",
    "    return sumo_config\n",
    "\n",
    "if \"SUMO_HOME\" in os.environ:\n",
    "    tools = os.path.join(os.environ[\"SUMO_HOME\"], \"tools\")\n",
    "    sys.path.append(tools)\n",
    "else:\n",
    "    sys.exit(\"Please declare environment variable 'SUMO_HOME'\")\n",
    "\n",
    "if not traci.isLoaded():\n",
    "    traci.start(sumo_config())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def save_episode_plots(metrics, combo_name=\"default\"):\n",
    "    os.makedirs(f\"Plots/{combo_name}\", exist_ok=True)\n",
    "\n",
    "    # Plot total reward per episode\n",
    "    plt.figure()\n",
    "    plt.plot(metrics[\"rewards_per_episode\"])\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Total Reward\")\n",
    "    plt.title(f\"{combo_name} - Reward per Episode\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"Plots/{combo_name}/rewards_per_episode.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Plot average wait per episode\n",
    "    plt.figure()\n",
    "    plt.plot(metrics[\"avg_wait_per_ep\"])\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Average Wait Time\")\n",
    "    plt.title(f\"{combo_name} - Avg Wait per Episode\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"Plots/{combo_name}/avg_wait_per_episode.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Plot maximum wait per episode\n",
    "    plt.figure()\n",
    "    plt.plot(metrics[\"max_wait_per_ep\"])\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Maximum Wait Time\")\n",
    "    plt.title(f\"{combo_name} - Max Wait per Episode\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"Plots/{combo_name}/max_wait_per_episode.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Per-lane average queue length plots\n",
    "    avg_queues = metrics[\"avg_queue_lengths\"]\n",
    "    for lane_index in range(avg_queues.shape[0]):\n",
    "        plt.figure()\n",
    "        plt.plot(avg_queues[lane_index].numpy())\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Average Queue Length\")\n",
    "        plt.title(f\"{combo_name} - Lane {lane_index} Avg Queue Length\")\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"Plots/{combo_name}/avg_queue_lane_{lane_index}.png\")\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Global Variables ---\n",
    "action_space_size = 8\n",
    "TRAFFIC_LIGHT_ID = \"traffic_light\"\n",
    "DELTA_PHASE_DURATION = 6\n",
    "YELLOW_PHASE_DURATION = 4\n",
    "lane_detectors = [f'q{i+1}' for i in range(8)]\n",
    "current_phase = 2\n",
    "\n",
    "def get_queue_length():\n",
    "    return torch.tensor([\n",
    "        traci.lanearea.getLastStepHaltingNumber(d) for d in lane_detectors\n",
    "    ], dtype=torch.float)\n",
    "\n",
    "def get_current_state():\n",
    "    return get_queue_length()\n",
    "\n",
    "def simulate_time(seconds=1):\n",
    "    for _ in range(20 * seconds):\n",
    "        traci.simulationStep()\n",
    "\n",
    "# def step(action):\n",
    "#     global current_phase\n",
    "#     if 2 * action == current_phase:\n",
    "#         traci.trafficlight.setPhase(TRAFFIC_LIGHT_ID, 2 * action)\n",
    "#         simulate_time(DELTA_PHASE_DURATION)\n",
    "#     else:\n",
    "#         traci.trafficlight.setPhase(TRAFFIC_LIGHT_ID, current_phase + 1)\n",
    "#         simulate_time(YELLOW_PHASE_DURATION)\n",
    "#         current_phase = 2 * action\n",
    "#         traci.trafficlight.setPhase(TRAFFIC_LIGHT_ID, 2 * action)\n",
    "#         simulate_time(DELTA_PHASE_DURATION)\n",
    "#     next_state = get_current_state()\n",
    "#     reward = -torch.sum(next_state)\n",
    "#     done = traci.simulation.getMinExpectedNumber() == 0\n",
    "#     return next_state, reward, done\n",
    "\n",
    "def step(action):\n",
    "    global current_phase\n",
    "    if 2 * action == current_phase:\n",
    "        traci.trafficlight.setPhase(TRAFFIC_LIGHT_ID, 2 * action)\n",
    "        simulate_time(DELTA_PHASE_DURATION)\n",
    "    else:\n",
    "        traci.trafficlight.setPhase(TRAFFIC_LIGHT_ID, current_phase + 1)\n",
    "        simulate_time(YELLOW_PHASE_DURATION)\n",
    "        current_phase = 2 * action\n",
    "        traci.trafficlight.setPhase(TRAFFIC_LIGHT_ID, 2 * action)\n",
    "        simulate_time(DELTA_PHASE_DURATION)\n",
    "        \n",
    "    next_state = get_current_state()\n",
    "    reward = -torch.sum(next_state)\n",
    "    done = traci.simulation.getMinExpectedNumber() == 0\n",
    "    return next_state, reward, done, next_state.clone()  # added queue tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Neural Network ---\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 64), nn.ReLU(),\n",
    "            nn.Linear(64, action_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Environment Switching ---\n",
    "import itertools\n",
    "traffic_patterns = itertools.cycle([\"P1\", \"P2\", \"P3\", \"P4\"])\n",
    "\n",
    "def change_env():\n",
    "    pattern = next(traffic_patterns)\n",
    "    if traci.isLoaded():\n",
    "        traci.close()\n",
    "    traci.start(sumo_config(pattern))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Choose Action ---\n",
    "def choose_action(state, epsilon, policy_net):\n",
    "    if random.random() < epsilon:\n",
    "        return random.randint(0, action_space_size - 1)\n",
    "    else:\n",
    "        return torch.argmax(policy_net(state.unsqueeze(0))).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Optimizer Step ---\n",
    "def optimise_model(policy_net, target_net, memory, optimizer, batch_size, gamma):\n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "    batch = random.sample(memory, batch_size)\n",
    "    states = torch.stack([x[0] for x in batch])\n",
    "    actions = torch.tensor([x[1] for x in batch]).unsqueeze(1)\n",
    "    rewards = torch.tensor([x[2] for x in batch], dtype=torch.float)\n",
    "    next_states = torch.stack([x[3] for x in batch])\n",
    "    dones = torch.tensor([x[4] for x in batch], dtype=torch.float)\n",
    "\n",
    "    q_vals = policy_net(states).gather(1, actions).squeeze()\n",
    "    with torch.no_grad():\n",
    "        max_next_q_vals = target_net(next_states).max(1)[0]\n",
    "        target_vals = rewards + gamma * max_next_q_vals * (1 - dones)\n",
    "    loss = nn.MSELoss()(q_vals, target_vals)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Training Loop ---\n",
    "# def train_algorithm(params, episodes=10):\n",
    "#     gamma = params[\"gamma\"]\n",
    "#     epsilon = params[\"epsilon\"]\n",
    "#     epsilon_decay = params[\"epsilon_decay\"]\n",
    "#     min_epsilon = params[\"min_epsilon\"]\n",
    "#     lr = params[\"learning_rate\"]\n",
    "#     batch_size = params[\"batch_size\"]\n",
    "#     target_update_freq = params[\"target_update_freq\"]\n",
    "#     memory_size = params[\"memory_size\"]\n",
    "\n",
    "#     state_dim = len(lane_detectors)\n",
    "#     policy_net = DQN(state_dim, action_space_size)\n",
    "#     target_net = DQN(state_dim, action_space_size)\n",
    "#     target_net.load_state_dict(policy_net.state_dict())\n",
    "#     optimizer = optim.Adam(policy_net.parameters(), lr=lr)\n",
    "#     memory = deque(maxlen=memory_size)\n",
    "\n",
    "#     rewards_per_episode = []\n",
    "#     steps_done = 0\n",
    "\n",
    "#     for episode in range(episodes):\n",
    "#         change_env()\n",
    "#         state = get_current_state()\n",
    "#         episode_reward = 0\n",
    "#         done = False\n",
    "\n",
    "#         while not done:\n",
    "#             action = choose_action(state, epsilon, policy_net)\n",
    "#             next_state, reward, done = step(action)\n",
    "#             memory.append((state, action, reward, next_state, done))\n",
    "#             state = next_state\n",
    "#             episode_reward += reward\n",
    "#             optimise_model(policy_net, target_net, memory, optimizer, batch_size, gamma)\n",
    "#             if steps_done % target_update_freq == 0:\n",
    "#                 target_net.load_state_dict(policy_net.state_dict())\n",
    "#             steps_done += 1\n",
    "\n",
    "#         epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "#         rewards_per_episode.append(episode_reward.item())\n",
    "\n",
    "#     return sum(rewards_per_episode) / episodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_algorithm(params, episodes=10):\n",
    "    gamma = params[\"gamma\"]\n",
    "    epsilon = params[\"epsilon\"]\n",
    "    epsilon_decay = params[\"epsilon_decay\"]\n",
    "    min_epsilon = params[\"min_epsilon\"]\n",
    "    lr = params[\"learning_rate\"]\n",
    "    batch_size = params[\"batch_size\"]\n",
    "    target_update_freq = params[\"target_update_freq\"]\n",
    "    memory_size = params[\"memory_size\"]\n",
    "\n",
    "    state_dim = len(lane_detectors)\n",
    "    policy_net = DQN(state_dim, action_space_size)\n",
    "    target_net = DQN(state_dim, action_space_size)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=lr)\n",
    "    memory = deque(maxlen=memory_size)\n",
    "\n",
    "    rewards_per_episode = []\n",
    "    steps_done = 0\n",
    "\n",
    "    avg_wait_per_ep = []\n",
    "    max_wait_per_ep = []\n",
    "    all_avg_queue_lengths = torch.zeros(len(lane_detectors), episodes)\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        if episode % 5 == 0:\n",
    "            print(f\"Episode: {episode}\")\n",
    "        change_env()\n",
    "        state = get_current_state()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "\n",
    "        vehicle_wait_tracker = {}\n",
    "        queue_length_tracker = {}\n",
    "        num_steps = 0\n",
    "\n",
    "        while not done:\n",
    "            action = choose_action(state, epsilon, policy_net)\n",
    "            next_state, reward, done, curr_queue = step(action)\n",
    "            memory.append((state, action, reward, next_state, done))\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "\n",
    "            # Vehicle wait time tracking\n",
    "            for v_id in traci.vehicle.getIDList():\n",
    "                wait_time = traci.vehicle.getWaitingTime(v_id)\n",
    "                if v_id not in vehicle_wait_tracker:\n",
    "                    vehicle_wait_tracker[v_id] = wait_time\n",
    "                elif wait_time > vehicle_wait_tracker[v_id]:\n",
    "                    vehicle_wait_tracker[v_id] = wait_time\n",
    "\n",
    "            # Queue length tracking\n",
    "            for i in range(len(curr_queue)):\n",
    "                queue_length_tracker[i] = queue_length_tracker.get(i, 0) + curr_queue[i]\n",
    "\n",
    "            optimise_model(policy_net, target_net, memory, optimizer, batch_size, gamma)\n",
    "            if steps_done % target_update_freq == 0:\n",
    "                target_net.load_state_dict(policy_net.state_dict())\n",
    "            steps_done += 1\n",
    "            num_steps += 1\n",
    "\n",
    "        # Aggregate queue stats\n",
    "        for i, total_len in queue_length_tracker.items():\n",
    "            all_avg_queue_lengths[i, episode] = total_len / num_steps\n",
    "\n",
    "        vehicle_waits = list(vehicle_wait_tracker.values())\n",
    "        avg_wait = sum(vehicle_waits) / len(vehicle_waits) if vehicle_waits else 0.0\n",
    "        max_wait = max(vehicle_waits) if vehicle_waits else 0.0\n",
    "        avg_wait_per_ep.append(avg_wait)\n",
    "        max_wait_per_ep.append(max_wait)\n",
    "\n",
    "        epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "        rewards_per_episode.append(episode_reward.item())\n",
    "\n",
    "    N = max(1, int(0.2 * episodes))  # 20% of episodes, at least 1\n",
    "    avg_reward_last_N = sum(rewards_per_episode[-N:]) / N\n",
    "    avg_wait_last_N = sum(avg_wait_per_ep[-N:]) / N\n",
    "    max_wait_last_N = max(max_wait_per_ep[-N:]) if max_wait_per_ep[-N:] else 0.0\n",
    "\n",
    "    return {\n",
    "        \"avg_reward\": sum(rewards_per_episode) / episodes,\n",
    "        \"avg_wait_per_ep\": avg_wait_per_ep,\n",
    "        \"max_wait_per_ep\": max_wait_per_ep,\n",
    "        \"avg_queue_lengths\": all_avg_queue_lengths,\n",
    "        \"rewards_per_episode\": rewards_per_episode,\n",
    "        \"avg_reward_last_N\": avg_reward_last_N,\n",
    "        \"avg_wait_last_N\": avg_wait_last_N,\n",
    "        \"max_wait_last_N\": max_wait_last_N,\n",
    "        \"trained_model\": policy_net\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "544ec7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Grid Search ---\n",
    "param_grid = {\n",
    "    \"gamma\": [0.99, 0.995, 0.999],  # All strong choices for long-term reward\n",
    "    \"epsilon\": [0.9],          # High initial exploration\n",
    "    \"epsilon_decay\": [0.95],  # Slow decay to retain exploration longer\n",
    "    \"min_epsilon\": [0.05],          # Fixed based on best practice\n",
    "    \"learning_rate\": [0.001],       # Fixed for stability on CPU\n",
    "    \"batch_size\": [128],        # Depending on CPU capacity\n",
    "    \"target_update_freq\": [1800],   # Once per episode\n",
    "    \"memory_size\": [10000, 20000]   # Moderate to ensure variety without overuse\n",
    "}\n",
    "\n",
    "keys, values = zip(*param_grid.items())\n",
    "param_combinations = [dict(zip(keys, v)) for v in product(*values)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = []\n",
    "\n",
    "# for combo in param_combinations:\n",
    "#     print(f\"Running combo: {combo}\")\n",
    "#     avg_reward = train_algorithm(combo, episodes=150)\n",
    "#     combo_result = combo.copy()\n",
    "#     combo_result[\"avg_reward\"] = avg_reward\n",
    "#     results.append(combo_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running combo: {'gamma': 0.99, 'epsilon': 0.9, 'epsilon_decay': 0.95, 'min_epsilon': 0.05, 'learning_rate': 0.001, 'batch_size': 128, 'target_update_freq': 1800, 'memory_size': 10000}\n",
      "Episode: 0\n",
      "Episode: 5\n",
      "Episode: 10\n",
      "Episode: 15\n",
      "Episode: 20\n",
      "Episode: 25\n",
      "Episode: 30\n",
      "Episode: 35\n",
      "Episode: 40\n",
      "Episode: 45\n",
      "Episode: 50\n",
      "Episode: 55\n",
      "Episode: 60\n",
      "Episode: 65\n",
      "Episode: 70\n",
      "Episode: 75\n",
      "Episode: 80\n",
      "Episode: 85\n",
      "Episode: 90\n",
      "Episode: 95\n",
      "Episode: 100\n",
      "Episode: 105\n",
      "Episode: 110\n",
      "Episode: 115\n",
      "Episode: 120\n",
      "Episode: 125\n",
      "Episode: 130\n",
      "Episode: 135\n",
      "Episode: 140\n",
      "Episode: 145\n",
      "Episode: 150\n",
      "Episode: 155\n",
      "Episode: 160\n",
      "Episode: 165\n",
      "Episode: 170\n",
      "Episode: 175\n",
      "Episode: 180\n",
      "Episode: 185\n",
      "Episode: 190\n",
      "Episode: 195\n",
      "Running combo: {'gamma': 0.99, 'epsilon': 0.9, 'epsilon_decay': 0.95, 'min_epsilon': 0.05, 'learning_rate': 0.001, 'batch_size': 128, 'target_update_freq': 1800, 'memory_size': 20000}\n",
      "Episode: 0\n",
      "Episode: 5\n",
      "Episode: 10\n",
      "Episode: 15\n",
      "Episode: 20\n",
      "Episode: 25\n",
      "Episode: 30\n",
      "Episode: 35\n",
      "Episode: 40\n",
      "Episode: 45\n",
      "Episode: 50\n",
      "Episode: 55\n",
      "Episode: 60\n",
      "Episode: 65\n",
      "Episode: 70\n",
      "Episode: 75\n",
      "Episode: 80\n",
      "Episode: 85\n",
      "Episode: 90\n",
      "Episode: 95\n",
      "Episode: 100\n",
      "Episode: 105\n",
      "Episode: 110\n",
      "Episode: 115\n",
      "Episode: 120\n",
      "Episode: 125\n",
      "Episode: 130\n",
      "Episode: 135\n",
      "Episode: 140\n",
      "Episode: 145\n",
      "Episode: 150\n",
      "Episode: 155\n",
      "Episode: 160\n",
      "Episode: 165\n",
      "Episode: 170\n",
      "Episode: 175\n",
      "Episode: 180\n",
      "Episode: 185\n",
      "Episode: 190\n",
      "Episode: 195\n",
      "Running combo: {'gamma': 0.995, 'epsilon': 0.9, 'epsilon_decay': 0.95, 'min_epsilon': 0.05, 'learning_rate': 0.001, 'batch_size': 128, 'target_update_freq': 1800, 'memory_size': 10000}\n",
      "Episode: 0\n",
      "Episode: 5\n",
      "Episode: 10\n",
      "Episode: 15\n",
      "Episode: 20\n",
      "Episode: 25\n",
      "Episode: 30\n",
      "Episode: 35\n",
      "Episode: 40\n",
      "Episode: 45\n",
      "Episode: 50\n",
      "Episode: 55\n",
      "Episode: 60\n",
      "Episode: 65\n",
      "Episode: 70\n",
      "Episode: 75\n",
      "Episode: 80\n",
      "Episode: 85\n",
      "Episode: 90\n",
      "Episode: 95\n",
      "Episode: 100\n",
      "Episode: 105\n",
      "Episode: 110\n",
      "Episode: 115\n",
      "Episode: 120\n",
      "Episode: 125\n",
      "Episode: 130\n",
      "Episode: 135\n",
      "Episode: 140\n",
      "Episode: 145\n",
      "Episode: 150\n",
      "Episode: 155\n",
      "Episode: 160\n",
      "Episode: 165\n",
      "Episode: 170\n",
      "Episode: 175\n",
      "Episode: 180\n",
      "Episode: 185\n",
      "Episode: 190\n",
      "Episode: 195\n",
      "Running combo: {'gamma': 0.995, 'epsilon': 0.9, 'epsilon_decay': 0.95, 'min_epsilon': 0.05, 'learning_rate': 0.001, 'batch_size': 128, 'target_update_freq': 1800, 'memory_size': 20000}\n",
      "Episode: 0\n",
      "Episode: 5\n",
      "Episode: 10\n",
      "Episode: 15\n",
      "Episode: 20\n",
      "Episode: 25\n",
      "Episode: 30\n",
      "Episode: 35\n",
      "Episode: 40\n",
      "Episode: 45\n",
      "Episode: 50\n",
      "Episode: 55\n",
      "Episode: 60\n",
      "Episode: 65\n",
      "Episode: 70\n",
      "Episode: 75\n",
      "Episode: 80\n",
      "Episode: 85\n",
      "Episode: 90\n",
      "Episode: 95\n",
      "Episode: 100\n",
      "Episode: 105\n",
      "Episode: 110\n",
      "Episode: 115\n",
      "Episode: 120\n",
      "Episode: 125\n",
      "Episode: 130\n",
      "Episode: 135\n",
      "Episode: 140\n",
      "Episode: 145\n",
      "Episode: 150\n",
      "Episode: 155\n",
      "Episode: 160\n",
      "Episode: 165\n",
      "Episode: 170\n",
      "Episode: 175\n",
      "Episode: 180\n",
      "Episode: 185\n",
      "Episode: 190\n",
      "Episode: 195\n",
      "Running combo: {'gamma': 0.999, 'epsilon': 0.9, 'epsilon_decay': 0.95, 'min_epsilon': 0.05, 'learning_rate': 0.001, 'batch_size': 128, 'target_update_freq': 1800, 'memory_size': 10000}\n",
      "Episode: 0\n",
      "Episode: 5\n",
      "Episode: 10\n",
      "Episode: 15\n",
      "Episode: 20\n",
      "Episode: 25\n",
      "Episode: 30\n",
      "Episode: 35\n",
      "Episode: 40\n",
      "Episode: 45\n",
      "Episode: 50\n",
      "Episode: 55\n",
      "Episode: 60\n",
      "Episode: 65\n",
      "Episode: 70\n",
      "Episode: 75\n",
      "Episode: 80\n",
      "Episode: 85\n",
      "Episode: 90\n",
      "Episode: 95\n",
      "Episode: 100\n",
      "Episode: 105\n",
      "Episode: 110\n",
      "Episode: 115\n",
      "Episode: 120\n",
      "Episode: 125\n",
      "Episode: 130\n",
      "Episode: 135\n",
      "Episode: 140\n",
      "Episode: 145\n",
      "Episode: 150\n",
      "Episode: 155\n",
      "Episode: 160\n",
      "Episode: 165\n",
      "Episode: 170\n",
      "Episode: 175\n",
      "Episode: 180\n",
      "Episode: 185\n",
      "Episode: 190\n",
      "Episode: 195\n",
      "Running combo: {'gamma': 0.999, 'epsilon': 0.9, 'epsilon_decay': 0.95, 'min_epsilon': 0.05, 'learning_rate': 0.001, 'batch_size': 128, 'target_update_freq': 1800, 'memory_size': 20000}\n",
      "Episode: 0\n",
      "Episode: 5\n",
      "Episode: 10\n",
      "Episode: 15\n",
      "Episode: 20\n",
      "Episode: 25\n",
      "Episode: 30\n",
      "Episode: 35\n",
      "Episode: 40\n",
      "Episode: 45\n",
      "Episode: 50\n",
      "Episode: 55\n",
      "Episode: 60\n",
      "Episode: 65\n",
      "Episode: 70\n",
      "Episode: 75\n",
      "Episode: 80\n",
      "Episode: 85\n",
      "Episode: 90\n",
      "Episode: 95\n",
      "Episode: 100\n",
      "Episode: 105\n",
      "Episode: 110\n",
      "Episode: 115\n",
      "Episode: 120\n",
      "Episode: 125\n",
      "Episode: 130\n",
      "Episode: 135\n",
      "Episode: 140\n",
      "Episode: 145\n",
      "Episode: 150\n",
      "Episode: 155\n",
      "Episode: 160\n",
      "Episode: 165\n",
      "Episode: 170\n",
      "Episode: 175\n",
      "Episode: 180\n",
      "Episode: 185\n",
      "Episode: 190\n",
      "Episode: 195\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for i, combo in enumerate(param_combinations):\n",
    "    print(f\"Running combo: {combo}\")\n",
    "    metrics = train_algorithm(combo, episodes=200)\n",
    "\n",
    "    # Save results\n",
    "    combo_result = combo.copy()\n",
    "    combo_result[\"avg_reward\"] = metrics[\"avg_reward\"]\n",
    "    combo_result[\"avg_reward_last_N\"] = metrics[\"avg_reward_last_N\"]\n",
    "    combo_result[\"avg_wait_last_N\"] = metrics[\"avg_wait_last_N\"]\n",
    "    combo_result[\"max_wait_last_N\"] = metrics[\"max_wait_last_N\"]\n",
    "    results.append(combo_result)\n",
    "\n",
    "    combo_name = f\"combo_{i+1}_\" + \"_\".join(f\"{k}-{v}\" for k, v in combo.items())\n",
    "\n",
    "    save_episode_plots(metrics, combo_name=combo_name)\n",
    "\n",
    "    os.makedirs(\"Trained_Models\", exist_ok=True)\n",
    "    model_path = f\"Trained_Models/{combo_name}.pt\"\n",
    "    torch.save(metrics[\"trained_model\"].state_dict(), model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(\"grid_search_results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   gamma  epsilon  epsilon_decay  min_epsilon  learning_rate  batch_size  \\\n",
      "1  0.990      0.9           0.97         0.05          0.001         128   \n",
      "0  0.990      0.9           0.97         0.05          0.001         128   \n",
      "5  0.995      0.9           0.97         0.05          0.001         128   \n",
      "4  0.995      0.9           0.97         0.05          0.001         128   \n",
      "6  0.995      0.9           0.99         0.05          0.001         128   \n",
      "\n",
      "   target_update_freq  memory_size  avg_reward  avg_reward_last_N  \\\n",
      "1                1800        20000   -1360.850          -1091.625   \n",
      "0                1800        10000   -1396.920          -1116.225   \n",
      "5                1800        20000   -1414.340          -1149.275   \n",
      "4                1800        10000   -1411.285          -1153.025   \n",
      "6                1800        10000   -1889.895          -1262.300   \n",
      "\n",
      "   avg_wait_last_N  max_wait_last_N  \n",
      "1         6.914892           293.50  \n",
      "0         7.095134           298.20  \n",
      "5         7.277027           298.50  \n",
      "4         7.278935           299.40  \n",
      "6         8.202031           292.65  \n"
     ]
    }
   ],
   "source": [
    "print(results_df.sort_values(\"avg_reward_last_N\", ascending=False).head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57e0837",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
