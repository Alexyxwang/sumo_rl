{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dqn_hyperparam_tuning.ipynb\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import traci\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SUMO Configuration ---\n",
    "def sumo_config(traffic_pattern=\"P1\"):\n",
    "    sumo_config = [\n",
    "        \"sumo\",\n",
    "        \"-c\", f\"SUMO_networks/{traffic_pattern}/junction.sumocfg\",\n",
    "        \"--step-length\", \"0.05\",\n",
    "        \"--delay\", \"0\",\n",
    "        \"--lateral-resolution\", \"0.1\",\n",
    "        \"--start\",\n",
    "        \"--no-warnings\",\n",
    "        \"--no-step-log\",\n",
    "    ]\n",
    "    return sumo_config\n",
    "\n",
    "if \"SUMO_HOME\" in os.environ:\n",
    "    tools = os.path.join(os.environ[\"SUMO_HOME\"], \"tools\")\n",
    "    sys.path.append(tools)\n",
    "else:\n",
    "    sys.exit(\"Please declare environment variable 'SUMO_HOME'\")\n",
    "\n",
    "if not traci.isLoaded():\n",
    "    traci.start(sumo_config())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def save_episode_plots(metrics, combo_name=\"default\"):\n",
    "    os.makedirs(f\"Plots/{combo_name}\", exist_ok=True)\n",
    "\n",
    "    # Plot total reward per episode\n",
    "    plt.figure()\n",
    "    plt.plot(metrics[\"rewards_per_episode\"])\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Total Reward\")\n",
    "    plt.title(f\"{combo_name} - Reward per Episode\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"Plots/{combo_name}/rewards_per_episode.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Plot average wait per episode\n",
    "    plt.figure()\n",
    "    plt.plot(metrics[\"avg_wait_per_ep\"])\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Average Wait Time\")\n",
    "    plt.title(f\"{combo_name} - Avg Wait per Episode\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"Plots/{combo_name}/avg_wait_per_episode.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Plot maximum wait per episode\n",
    "    plt.figure()\n",
    "    plt.plot(metrics[\"max_wait_per_ep\"])\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Maximum Wait Time\")\n",
    "    plt.title(f\"{combo_name} - Max Wait per Episode\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"Plots/{combo_name}/max_wait_per_episode.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Per-lane average queue length plots\n",
    "    avg_queues = metrics[\"avg_queue_lengths\"]\n",
    "    for lane_index in range(avg_queues.shape[0]):\n",
    "        plt.figure()\n",
    "        plt.plot(avg_queues[lane_index].numpy())\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Average Queue Length\")\n",
    "        plt.title(f\"{combo_name} - Lane {lane_index} Avg Queue Length\")\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"Plots/{combo_name}/avg_queue_lane_{lane_index}.png\")\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Global Variables ---\n",
    "action_space_size = 8\n",
    "TRAFFIC_LIGHT_ID = \"traffic_light\"\n",
    "DELTA_PHASE_DURATION = 6\n",
    "YELLOW_PHASE_DURATION = 4\n",
    "lane_detectors = [f'q{i+1}' for i in range(8)]\n",
    "current_phase = 2\n",
    "\n",
    "def get_queue_length():\n",
    "    return torch.tensor([\n",
    "        traci.lanearea.getLastStepHaltingNumber(d) for d in lane_detectors\n",
    "    ], dtype=torch.float)\n",
    "\n",
    "def get_current_state():\n",
    "    return get_queue_length()\n",
    "\n",
    "def simulate_time(seconds=1):\n",
    "    for _ in range(20 * seconds):\n",
    "        traci.simulationStep()\n",
    "\n",
    "# def step(action):\n",
    "#     global current_phase\n",
    "#     if 2 * action == current_phase:\n",
    "#         traci.trafficlight.setPhase(TRAFFIC_LIGHT_ID, 2 * action)\n",
    "#         simulate_time(DELTA_PHASE_DURATION)\n",
    "#     else:\n",
    "#         traci.trafficlight.setPhase(TRAFFIC_LIGHT_ID, current_phase + 1)\n",
    "#         simulate_time(YELLOW_PHASE_DURATION)\n",
    "#         current_phase = 2 * action\n",
    "#         traci.trafficlight.setPhase(TRAFFIC_LIGHT_ID, 2 * action)\n",
    "#         simulate_time(DELTA_PHASE_DURATION)\n",
    "#     next_state = get_current_state()\n",
    "#     reward = -torch.sum(next_state)\n",
    "#     done = traci.simulation.getMinExpectedNumber() == 0\n",
    "#     return next_state, reward, done\n",
    "\n",
    "def step(action):\n",
    "    global current_phase\n",
    "    if 2 * action == current_phase:\n",
    "        traci.trafficlight.setPhase(TRAFFIC_LIGHT_ID, 2 * action)\n",
    "        simulate_time(DELTA_PHASE_DURATION)\n",
    "    else:\n",
    "        traci.trafficlight.setPhase(TRAFFIC_LIGHT_ID, current_phase + 1)\n",
    "        simulate_time(YELLOW_PHASE_DURATION)\n",
    "        current_phase = 2 * action\n",
    "        traci.trafficlight.setPhase(TRAFFIC_LIGHT_ID, 2 * action)\n",
    "        simulate_time(DELTA_PHASE_DURATION)\n",
    "        \n",
    "    next_state = get_current_state()\n",
    "    reward = -torch.sum(next_state)\n",
    "    done = traci.simulation.getMinExpectedNumber() == 0\n",
    "    return next_state, reward, done, next_state.clone()  # added queue tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Neural Network ---\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 64), nn.ReLU(),\n",
    "            nn.Linear(64, action_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Environment Switching ---\n",
    "import itertools\n",
    "traffic_patterns = itertools.cycle([\"P1\", \"P2\", \"P3\", \"P4\"])\n",
    "\n",
    "def change_env():\n",
    "    pattern = next(traffic_patterns)\n",
    "    if traci.isLoaded():\n",
    "        traci.close()\n",
    "    traci.start(sumo_config(pattern))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Choose Action ---\n",
    "def choose_action(state, epsilon, policy_net):\n",
    "    if random.random() < epsilon:\n",
    "        return random.randint(0, action_space_size - 1)\n",
    "    else:\n",
    "        return torch.argmax(policy_net(state.unsqueeze(0))).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Optimizer Step ---\n",
    "def optimise_model(policy_net, target_net, memory, optimizer, batch_size, gamma):\n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "    batch = random.sample(memory, batch_size)\n",
    "    states = torch.stack([x[0] for x in batch])\n",
    "    actions = torch.tensor([x[1] for x in batch]).unsqueeze(1)\n",
    "    rewards = torch.tensor([x[2] for x in batch], dtype=torch.float)\n",
    "    next_states = torch.stack([x[3] for x in batch])\n",
    "    dones = torch.tensor([x[4] for x in batch], dtype=torch.float)\n",
    "\n",
    "    q_vals = policy_net(states).gather(1, actions).squeeze()\n",
    "    with torch.no_grad():\n",
    "        max_next_q_vals = target_net(next_states).max(1)[0]\n",
    "        target_vals = rewards + gamma * max_next_q_vals * (1 - dones)\n",
    "    loss = nn.MSELoss()(q_vals, target_vals)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Training Loop ---\n",
    "# def train_algorithm(params, episodes=10):\n",
    "#     gamma = params[\"gamma\"]\n",
    "#     epsilon = params[\"epsilon\"]\n",
    "#     epsilon_decay = params[\"epsilon_decay\"]\n",
    "#     min_epsilon = params[\"min_epsilon\"]\n",
    "#     lr = params[\"learning_rate\"]\n",
    "#     batch_size = params[\"batch_size\"]\n",
    "#     target_update_freq = params[\"target_update_freq\"]\n",
    "#     memory_size = params[\"memory_size\"]\n",
    "\n",
    "#     state_dim = len(lane_detectors)\n",
    "#     policy_net = DQN(state_dim, action_space_size)\n",
    "#     target_net = DQN(state_dim, action_space_size)\n",
    "#     target_net.load_state_dict(policy_net.state_dict())\n",
    "#     optimizer = optim.Adam(policy_net.parameters(), lr=lr)\n",
    "#     memory = deque(maxlen=memory_size)\n",
    "\n",
    "#     rewards_per_episode = []\n",
    "#     steps_done = 0\n",
    "\n",
    "#     for episode in range(episodes):\n",
    "#         change_env()\n",
    "#         state = get_current_state()\n",
    "#         episode_reward = 0\n",
    "#         done = False\n",
    "\n",
    "#         while not done:\n",
    "#             action = choose_action(state, epsilon, policy_net)\n",
    "#             next_state, reward, done = step(action)\n",
    "#             memory.append((state, action, reward, next_state, done))\n",
    "#             state = next_state\n",
    "#             episode_reward += reward\n",
    "#             optimise_model(policy_net, target_net, memory, optimizer, batch_size, gamma)\n",
    "#             if steps_done % target_update_freq == 0:\n",
    "#                 target_net.load_state_dict(policy_net.state_dict())\n",
    "#             steps_done += 1\n",
    "\n",
    "#         epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "#         rewards_per_episode.append(episode_reward.item())\n",
    "\n",
    "#     return sum(rewards_per_episode) / episodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_algorithm(params, episodes=10):\n",
    "    gamma = params[\"gamma\"]\n",
    "    epsilon = params[\"epsilon\"]\n",
    "    epsilon_decay = params[\"epsilon_decay\"]\n",
    "    min_epsilon = params[\"min_epsilon\"]\n",
    "    lr = params[\"learning_rate\"]\n",
    "    batch_size = params[\"batch_size\"]\n",
    "    target_update_freq = params[\"target_update_freq\"]\n",
    "    memory_size = params[\"memory_size\"]\n",
    "\n",
    "    state_dim = len(lane_detectors)\n",
    "    policy_net = DQN(state_dim, action_space_size)\n",
    "    target_net = DQN(state_dim, action_space_size)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=lr)\n",
    "    memory = deque(maxlen=memory_size)\n",
    "\n",
    "    rewards_per_episode = []\n",
    "    steps_done = 0\n",
    "\n",
    "    avg_wait_per_ep = []\n",
    "    max_wait_per_ep = []\n",
    "    all_avg_queue_lengths = torch.zeros(len(lane_detectors), episodes)\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        if episode % 5 == 0:\n",
    "            print(f\"Episode: {episode}\")\n",
    "        change_env()\n",
    "        state = get_current_state()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "\n",
    "        vehicle_wait_tracker = {}\n",
    "        queue_length_tracker = {}\n",
    "        num_steps = 0\n",
    "\n",
    "        while not done:\n",
    "            action = choose_action(state, epsilon, policy_net)\n",
    "            next_state, reward, done, curr_queue = step(action)\n",
    "            memory.append((state, action, reward, next_state, done))\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "\n",
    "            # Vehicle wait time tracking\n",
    "            for v_id in traci.vehicle.getIDList():\n",
    "                wait_time = traci.vehicle.getWaitingTime(v_id)\n",
    "                if v_id not in vehicle_wait_tracker:\n",
    "                    vehicle_wait_tracker[v_id] = wait_time\n",
    "                elif wait_time > vehicle_wait_tracker[v_id]:\n",
    "                    vehicle_wait_tracker[v_id] = wait_time\n",
    "\n",
    "            # Queue length tracking\n",
    "            for i in range(len(curr_queue)):\n",
    "                queue_length_tracker[i] = queue_length_tracker.get(i, 0) + curr_queue[i]\n",
    "\n",
    "            optimise_model(policy_net, target_net, memory, optimizer, batch_size, gamma)\n",
    "            if steps_done % target_update_freq == 0:\n",
    "                target_net.load_state_dict(policy_net.state_dict())\n",
    "            steps_done += 1\n",
    "            num_steps += 1\n",
    "\n",
    "        # Aggregate queue stats\n",
    "        for i, total_len in queue_length_tracker.items():\n",
    "            all_avg_queue_lengths[i, episode] = total_len / num_steps\n",
    "\n",
    "        vehicle_waits = list(vehicle_wait_tracker.values())\n",
    "        avg_wait = sum(vehicle_waits) / len(vehicle_waits) if vehicle_waits else 0.0\n",
    "        max_wait = max(vehicle_waits) if vehicle_waits else 0.0\n",
    "        avg_wait_per_ep.append(avg_wait)\n",
    "        max_wait_per_ep.append(max_wait)\n",
    "\n",
    "        epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "        rewards_per_episode.append(episode_reward.item())\n",
    "\n",
    "    N = max(1, int(0.2 * episodes))  # 20% of episodes, at least 1\n",
    "    avg_reward_last_N = sum(rewards_per_episode[-N:]) / N\n",
    "    avg_wait_last_N = sum(avg_wait_per_ep[-N:]) / N\n",
    "    max_wait_last_N = max(max_wait_per_ep[-N:]) if max_wait_per_ep[-N:] else 0.0\n",
    "\n",
    "    return {\n",
    "        \"avg_reward\": sum(rewards_per_episode) / episodes,\n",
    "        \"avg_wait_per_ep\": avg_wait_per_ep,\n",
    "        \"max_wait_per_ep\": max_wait_per_ep,\n",
    "        \"avg_queue_lengths\": all_avg_queue_lengths,\n",
    "        \"rewards_per_episode\": rewards_per_episode,\n",
    "        \"avg_reward_last_N\": avg_reward_last_N,\n",
    "        \"avg_wait_last_N\": avg_wait_last_N,\n",
    "        \"max_wait_last_N\": max_wait_last_N,\n",
    "        \"trained_model\": policy_net\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "544ec7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Grid Search ---\n",
    "param_grid = {\n",
    "    \"gamma\": [0.99, 0.995, 0.999],  # All strong choices for long-term reward\n",
    "    \"epsilon\": [0.9],          # High initial exploration\n",
    "    \"epsilon_decay\": [0.97, 0.99],  # Slow decay to retain exploration longer\n",
    "    \"min_epsilon\": [0.05],          # Fixed based on best practice\n",
    "    \"learning_rate\": [0.001],       # Fixed for stability on CPU\n",
    "    \"batch_size\": [128],        # Depending on CPU capacity\n",
    "    \"target_update_freq\": [1800],   # Once per episode\n",
    "    \"memory_size\": [10000, 20000]   # Moderate to ensure variety without overuse\n",
    "}\n",
    "\n",
    "keys, values = zip(*param_grid.items())\n",
    "param_combinations = [dict(zip(keys, v)) for v in product(*values)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = []\n",
    "\n",
    "# for combo in param_combinations:\n",
    "#     print(f\"Running combo: {combo}\")\n",
    "#     avg_reward = train_algorithm(combo, episodes=150)\n",
    "#     combo_result = combo.copy()\n",
    "#     combo_result[\"avg_reward\"] = avg_reward\n",
    "#     results.append(combo_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running combo: {'gamma': 0.99, 'epsilon': 0.9, 'epsilon_decay': 0.97, 'min_epsilon': 0.05, 'learning_rate': 0.001, 'batch_size': 128, 'target_update_freq': 1800, 'memory_size': 10000}\n",
      "Episode: 0\n",
      "Episode: 5\n",
      "Episode: 10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, combo \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(param_combinations):\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRunning combo: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcombo\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     metrics = \u001b[43mtrain_algorithm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m     \u001b[38;5;66;03m# Save results\u001b[39;00m\n\u001b[32m      8\u001b[39m     combo_result = combo.copy()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 39\u001b[39m, in \u001b[36mtrain_algorithm\u001b[39m\u001b[34m(params, episodes)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[32m     38\u001b[39m     action = choose_action(state, epsilon, policy_net)\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     next_state, reward, done, curr_queue = \u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m     memory.append((state, action, reward, next_state, done))\n\u001b[32m     41\u001b[39m     state = next_state\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 47\u001b[39m, in \u001b[36mstep\u001b[39m\u001b[34m(action)\u001b[39m\n\u001b[32m     45\u001b[39m     current_phase = \u001b[32m2\u001b[39m * action\n\u001b[32m     46\u001b[39m     traci.trafficlight.setPhase(TRAFFIC_LIGHT_ID, \u001b[32m2\u001b[39m * action)\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     \u001b[43msimulate_time\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDELTA_PHASE_DURATION\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m next_state = get_current_state()\n\u001b[32m     50\u001b[39m reward = -torch.sum(next_state)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36msimulate_time\u001b[39m\u001b[34m(seconds)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msimulate_time\u001b[39m(seconds=\u001b[32m1\u001b[39m):\n\u001b[32m     18\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m20\u001b[39m * seconds):\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m         \u001b[43mtraci\u001b[49m\u001b[43m.\u001b[49m\u001b[43msimulationStep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\main.py:200\u001b[39m, in \u001b[36msimulationStep\u001b[39m\u001b[34m(step)\u001b[39m\n\u001b[32m    194\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msimulationStep\u001b[39m(step=\u001b[32m0\u001b[39m):\n\u001b[32m    195\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"simulationStep(float) -> None\u001b[39;00m\n\u001b[32m    196\u001b[39m \u001b[33;03m    Make a simulation step and simulate up to the given second in sim time.\u001b[39;00m\n\u001b[32m    197\u001b[39m \u001b[33;03m    If the given value is 0 or absent, exactly one step is performed.\u001b[39;00m\n\u001b[32m    198\u001b[39m \u001b[33;03m    Values smaller than or equal to the current sim time result in no action.\u001b[39;00m\n\u001b[32m    199\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m     \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msimulationStep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\connection.py:370\u001b[39m, in \u001b[36mConnection.simulationStep\u001b[39m\u001b[34m(self, step)\u001b[39m\n\u001b[32m    368\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(step) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mint\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m step >= \u001b[32m1000\u001b[39m:\n\u001b[32m    369\u001b[39m     warnings.warn(\u001b[33m\"\u001b[39m\u001b[33mAPI change now handles step as floating point seconds\u001b[39m\u001b[33m\"\u001b[39m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m370\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sendCmd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCMD_SIMSTEP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mD\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m subscriptionResults \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._subscriptionMapping.values():\n\u001b[32m    372\u001b[39m     subscriptionResults.reset()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\connection.py:232\u001b[39m, in \u001b[36mConnection._sendCmd\u001b[39m\u001b[34m(self, cmdID, varID, objID, format, *values)\u001b[39m\n\u001b[32m    230\u001b[39m     \u001b[38;5;28mself\u001b[39m._string += struct.pack(\u001b[33m\"\u001b[39m\u001b[33m!i\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(objID)) + objID\n\u001b[32m    231\u001b[39m \u001b[38;5;28mself\u001b[39m._string += packed\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sendExact\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\connection.py:131\u001b[39m, in \u001b[36mConnection._sendExact\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    129\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33msending\u001b[39m\u001b[33m\"\u001b[39m, Storage(length + \u001b[38;5;28mself\u001b[39m._string).getDebugString())\n\u001b[32m    130\u001b[39m \u001b[38;5;28mself\u001b[39m._socket.send(length + \u001b[38;5;28mself\u001b[39m._string)\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_recvExact\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _DEBUG:\n\u001b[32m    133\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mreceiving\u001b[39m\u001b[33m\"\u001b[39m, result.getDebugString())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\connection.py:109\u001b[39m, in \u001b[36mConnection._recvExact\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    107\u001b[39m result = \u001b[38;5;28mbytes\u001b[39m()\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result) < \u001b[32m4\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m     t = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_socket\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    110\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t:\n\u001b[32m    111\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for i, combo in enumerate(param_combinations):\n",
    "    print(f\"Running combo: {combo}\")\n",
    "    metrics = train_algorithm(combo, episodes=200)\n",
    "\n",
    "    # Save results\n",
    "    combo_result = combo.copy()\n",
    "    combo_result[\"avg_reward\"] = metrics[\"avg_reward\"]\n",
    "    combo_result[\"avg_reward_last_N\"] = metrics[\"avg_reward_last_N\"]\n",
    "    combo_result[\"avg_wait_last_N\"] = metrics[\"avg_wait_last_N\"]\n",
    "    combo_result[\"max_wait_last_N\"] = metrics[\"max_wait_last_N\"]\n",
    "    results.append(combo_result)\n",
    "\n",
    "    combo_name = f\"combo_{i+1}_\" + \"_\".join(f\"{k}-{v}\" for k, v in combo.items())\n",
    "\n",
    "    save_episode_plots(metrics, combo_name=combo_name)\n",
    "\n",
    "    os.makedirs(\"Trained_Models\", exist_ok=True)\n",
    "    model_path = f\"Trained_Models/{combo_name}.pt\"\n",
    "    torch.save(metrics[\"trained_model\"].state_dict(), model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(\"grid_search_results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   gamma  epsilon  epsilon_decay  min_epsilon  learning_rate  batch_size  \\\n",
      "0   0.99      0.9           0.95         0.05          0.001         128   \n",
      "\n",
      "   target_update_freq  memory_size  avg_reward  avg_reward_last_N  \\\n",
      "0                 400        10000     -8452.0            -8452.0   \n",
      "\n",
      "   avg_wait_last_N  max_wait_last_N  \n",
      "0        28.311547           168.15  \n"
     ]
    }
   ],
   "source": [
    "print(results_df.sort_values(\"avg_reward_last_N\", ascending=False).head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
