{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall torch\n",
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9473567c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import traci\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "import pandas as pd\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Using device:\", device)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if device.type == \"cuda\":\n",
    "    print(\"Device name:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% --- SUMO Configuration ---\n",
    "def sumo_config(traffic_pattern=\"P1\"):\n",
    "    return [\n",
    "        \"sumo\",\n",
    "        \"-c\", f\"SUMO_networks/{traffic_pattern}/junction.sumocfg\",\n",
    "        \"--step-length\", \"0.05\",\n",
    "        \"--delay\", \"0\",\n",
    "        \"--lateral-resolution\", \"0.1\",\n",
    "        \"--start\",\n",
    "        \"--no-warnings\",\n",
    "        \"--no-step-log\",\n",
    "    ]\n",
    "\n",
    "if \"SUMO_HOME\" in os.environ:\n",
    "    tools = os.path.join(os.environ[\"SUMO_HOME\"], \"tools\")\n",
    "    sys.path.append(tools)\n",
    "else:\n",
    "    sys.exit(\"Please declare environment variable 'SUMO_HOME'\")\n",
    "\n",
    "if not traci.isLoaded():\n",
    "    traci.start(sumo_config())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% --- Global Variables ---\n",
    "action_space_size = 8\n",
    "TRAFFIC_LIGHT_ID = \"traffic_light\"\n",
    "DELTA_PHASE_DURATION = 6\n",
    "YELLOW_PHASE_DURATION = 4\n",
    "lane_detectors = [f'q{i+1}' for i in range(8)]\n",
    "current_phase = 2\n",
    "\n",
    "def get_queue_length():\n",
    "    return torch.tensor([\n",
    "        traci.lanearea.getLastStepHaltingNumber(d) for d in lane_detectors\n",
    "    ], dtype=torch.float)\n",
    "\n",
    "def get_current_state():\n",
    "    return get_queue_length()\n",
    "\n",
    "def simulate_time(seconds=1):\n",
    "    for _ in range(20 * seconds):\n",
    "        traci.simulationStep()\n",
    "\n",
    "def step(action):\n",
    "    global current_phase\n",
    "    if 2 * action == current_phase:\n",
    "        traci.trafficlight.setPhase(TRAFFIC_LIGHT_ID, 2 * action)\n",
    "        simulate_time(DELTA_PHASE_DURATION)\n",
    "    else:\n",
    "        traci.trafficlight.setPhase(TRAFFIC_LIGHT_ID, current_phase + 1)\n",
    "        simulate_time(YELLOW_PHASE_DURATION)\n",
    "        current_phase = 2 * action\n",
    "        traci.trafficlight.setPhase(TRAFFIC_LIGHT_ID, 2 * action)\n",
    "        simulate_time(DELTA_PHASE_DURATION)\n",
    "\n",
    "    next_state = get_current_state()\n",
    "    reward = -torch.sum(next_state)\n",
    "    done = traci.simulation.getMinExpectedNumber() == 0\n",
    "    return next_state, reward, done, next_state.clone()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% --- Neural Network ---\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 64), nn.ReLU(),\n",
    "            nn.Linear(64, action_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% --- Environment Switching ---\n",
    "traffic_patterns = itertools.cycle([\"P1\", \"P2\", \"P3\", \"P4\"])\n",
    "\n",
    "def change_env():\n",
    "    pattern = next(traffic_patterns)\n",
    "    if traci.isLoaded():\n",
    "        traci.close()\n",
    "    traci.start(sumo_config(pattern))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% --- Choose Action ---\n",
    "def choose_action(state, epsilon, policy_net):\n",
    "    if random.random() < epsilon:\n",
    "        return random.randint(0, action_space_size - 1)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            return torch.argmax(policy_net(state.unsqueeze(0).to(device))).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% --- Optimizer Step ---\n",
    "def optimise_model(policy_net, target_net, memory, optimizer, batch_size, gamma):\n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "    batch = random.sample(memory, batch_size)\n",
    "    states = torch.stack([x[0] for x in batch]).to(device)\n",
    "    actions = torch.tensor([x[1] for x in batch]).unsqueeze(1).to(device)\n",
    "    rewards = torch.tensor([x[2] for x in batch], dtype=torch.float).to(device)\n",
    "    next_states = torch.stack([x[3] for x in batch]).to(device)\n",
    "    dones = torch.tensor([x[4] for x in batch], dtype=torch.float).to(device)\n",
    "\n",
    "    q_vals = policy_net(states).gather(1, actions).squeeze()\n",
    "    with torch.no_grad():\n",
    "        max_next_q_vals = target_net(next_states).max(1)[0]\n",
    "        target_vals = rewards + gamma * max_next_q_vals * (1 - dones)\n",
    "    loss = nn.MSELoss()(q_vals, target_vals)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% --- Training Loop ---\n",
    "def train_algorithm(params, episodes=10):\n",
    "    gamma = params[\"gamma\"]\n",
    "    epsilon = params[\"epsilon\"]\n",
    "    epsilon_decay = params[\"epsilon_decay\"]\n",
    "    min_epsilon = params[\"min_epsilon\"]\n",
    "    lr = params[\"learning_rate\"]\n",
    "    batch_size = params[\"batch_size\"]\n",
    "    target_update_freq = params[\"target_update_freq\"]\n",
    "    memory_size = params[\"memory_size\"]\n",
    "\n",
    "    state_dim = len(lane_detectors)\n",
    "    policy_net = DQN(state_dim, action_space_size).to(device)\n",
    "    target_net = DQN(state_dim, action_space_size).to(device)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=lr)\n",
    "    memory = deque(maxlen=memory_size)\n",
    "    \n",
    "    rewards_per_episode = []\n",
    "    steps_done = 0\n",
    "\n",
    "    avg_wait_per_ep = []\n",
    "    max_wait_per_ep = []\n",
    "    all_avg_queue_lengths = torch.zeros(len(lane_detectors), episodes)\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        change_env()\n",
    "        state = get_current_state().to(device)\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "\n",
    "        vehicle_wait_tracker = {}\n",
    "        queue_length_tracker = {}\n",
    "        num_steps = 0\n",
    "\n",
    "        while not done:\n",
    "            action = choose_action(state, epsilon, policy_net)\n",
    "            next_state, reward, done, curr_queue = step(action)\n",
    "            next_state = next_state.to(device)\n",
    "            memory.append((state, action, reward, next_state, done))\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "\n",
    "            # print(\"Policy Net device:\", next(policy_net.parameters()).device)\n",
    "            # print(\"State device:\", state.device)\n",
    "    \n",
    "            for v_id in traci.vehicle.getIDList():\n",
    "                wait_time = traci.vehicle.getWaitingTime(v_id)\n",
    "                if v_id not in vehicle_wait_tracker:\n",
    "                    vehicle_wait_tracker[v_id] = wait_time\n",
    "                elif wait_time > vehicle_wait_tracker[v_id]:\n",
    "                    vehicle_wait_tracker[v_id] = wait_time\n",
    "\n",
    "            for i in range(len(curr_queue)):\n",
    "                queue_length_tracker[i] = queue_length_tracker.get(i, 0) + curr_queue[i]\n",
    "\n",
    "            optimise_model(policy_net, target_net, memory, optimizer, batch_size, gamma)\n",
    "            if steps_done % target_update_freq == 0:\n",
    "                target_net.load_state_dict(policy_net.state_dict())\n",
    "            steps_done += 1\n",
    "            num_steps += 1\n",
    "\n",
    "        for i, total_len in queue_length_tracker.items():\n",
    "            all_avg_queue_lengths[i, episode] = total_len / num_steps\n",
    "\n",
    "        vehicle_waits = list(vehicle_wait_tracker.values())\n",
    "        avg_wait = sum(vehicle_waits) / len(vehicle_waits) if vehicle_waits else 0.0\n",
    "        max_wait = max(vehicle_waits) if vehicle_waits else 0.0\n",
    "        avg_wait_per_ep.append(avg_wait)\n",
    "        max_wait_per_ep.append(max_wait)\n",
    "\n",
    "        epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "        rewards_per_episode.append(episode_reward.item())\n",
    "\n",
    "    N = max(1, int(0.2 * episodes))\n",
    "    avg_reward_last_N = sum(rewards_per_episode[-N:]) / N\n",
    "    avg_wait_last_N = sum(avg_wait_per_ep[-N:]) / N\n",
    "    max_wait_last_N = max(max_wait_per_ep[-N:]) if max_wait_per_ep[-N:] else 0.0\n",
    "\n",
    "    return {\n",
    "        \"avg_reward\": sum(rewards_per_episode) / episodes,\n",
    "        \"avg_wait_per_ep\": avg_wait_per_ep,\n",
    "        \"max_wait_per_ep\": max_wait_per_ep,\n",
    "        \"avg_queue_lengths\": all_avg_queue_lengths,\n",
    "        \"rewards_per_episode\": rewards_per_episode,\n",
    "        \"avg_reward_last_N\": avg_reward_last_N,\n",
    "        \"avg_wait_last_N\": avg_wait_last_N,\n",
    "        \"max_wait_last_N\": max_wait_last_N,\n",
    "        \"trained_model\": policy_net\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% --- Parameter Grid ---\n",
    "param_grid = {\n",
    "    \"gamma\": [0.99],\n",
    "    \"epsilon\": [0.9],\n",
    "    \"epsilon_decay\": [0.95],\n",
    "    \"min_epsilon\": [0.05],\n",
    "    \"learning_rate\": [0.001],\n",
    "    \"batch_size\": [128],\n",
    "    \"target_update_freq\": [400],\n",
    "    \"memory_size\": [10000],\n",
    "}\n",
    "\n",
    "keys, values = zip(*param_grid.items())\n",
    "param_combinations = [dict(zip(keys, v)) for v in product(*values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% --- Plot Saving Function ---\n",
    "def save_episode_plots(metrics, combo_name=\"default\"):\n",
    "    os.makedirs(f\"Plots/{combo_name}\", exist_ok=True)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(metrics[\"rewards_per_episode\"])\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Total Reward\")\n",
    "    plt.title(f\"{combo_name} - Reward per Episode\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"Plots/{combo_name}/rewards_per_episode.png\")\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(metrics[\"avg_wait_per_ep\"])\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Average Wait Time\")\n",
    "    plt.title(f\"{combo_name} - Avg Wait per Episode\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"Plots/{combo_name}/avg_wait_per_episode.png\")\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(metrics[\"max_wait_per_ep\"])\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Maximum Wait Time\")\n",
    "    plt.title(f\"{combo_name} - Max Wait per Episode\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"Plots/{combo_name}/max_wait_per_episode.png\")\n",
    "    plt.close()\n",
    "\n",
    "    avg_queues = metrics[\"avg_queue_lengths\"]\n",
    "    for lane_index in range(avg_queues.shape[0]):\n",
    "        plt.figure()\n",
    "        plt.plot(avg_queues[lane_index].cpu().numpy())\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Average Queue Length\")\n",
    "        plt.title(f\"{combo_name} - Lane {lane_index} Avg Queue Length\")\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"Plots/{combo_name}/avg_queue_lane_{lane_index}.png\")\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "results = Parallel(n_jobs=4)(\n",
    "    delayed(train_algorithm)(combo, episodes=4)\n",
    "    for combo in param_combinations\n",
    ")\n",
    "\n",
    "# Collect results with parameter info\n",
    "final_results = []\n",
    "for combo, metrics in zip(param_combinations, results):\n",
    "    combo_result = combo.copy()\n",
    "    combo_result[\"avg_reward\"] = metrics[\"avg_reward\"]\n",
    "    combo_result[\"avg_reward_last_N\"] = metrics[\"avg_reward_last_N\"]\n",
    "    combo_result[\"avg_wait_last_N\"] = metrics[\"avg_wait_last_N\"]\n",
    "    combo_result[\"max_wait_last_N\"] = metrics[\"max_wait_last_N\"]\n",
    "    final_results.append(combo_result)\n",
    "\n",
    "    combo_name = f\"combo_\" + \"_\".join(f\"{k}-{v}\" for k, v in combo.items())\n",
    "    save_episode_plots(metrics, combo_name=combo_name)\n",
    "\n",
    "    model_path = f\"Trained_Models/{combo_name}.pt\"\n",
    "    torch.save(metrics[\"trained_model\"].state_dict(), model_path)\n",
    "\n",
    "results_df = pd.DataFrame(final_results)\n",
    "results_df.to_csv(\"grid_search_results.csv\", index=False)\n",
    "\n",
    "print(results_df.sort_values(\"avg_reward_last_N\", ascending=False).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %% --- Save Results ---\n",
    "# results_df = pd.DataFrame(results)\n",
    "# results_df.to_csv(\"grid_search_results.csv\", index=False)\n",
    "\n",
    "# # %% --- Print Best Results ---\n",
    "# print(results_df.sort_values(\"avg_reward_last_N\", ascending=False).head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "9444_group_assignment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
