{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40794afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import traci\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c1eca3",
   "metadata": {},
   "source": [
    "## **SUMO Config**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e400644",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sumo_config(traffic_pattern=\"P1\"):\n",
    "    sumo_config = [\n",
    "        \"sumo\",\n",
    "        \"-c\", f\"SUMO_networks/{traffic_pattern}/junction.sumocfg\",\n",
    "        \"--step-length\", \"0.05\",\n",
    "        \"--delay\", \"0\",\n",
    "        \"--lateral-resolution\", \"0.1\",\n",
    "        \"--start\",\n",
    "        \"--no-warnings\",\n",
    "        \"--no-step-log\",\n",
    "    ]\n",
    "    return sumo_config\n",
    "\n",
    "if \"SUMO_HOME\" in os.environ:\n",
    "    tools = os.path.join(os.environ[\"SUMO_HOME\"], \"tools\")\n",
    "    sys.path.append(tools)\n",
    "else:\n",
    "    sys.exit(\"Declare SUMO_HOME env variable\")\n",
    "\n",
    "if not traci.isLoaded():\n",
    "    traci.start(sumo_config())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc07747",
   "metadata": {},
   "source": [
    "## **Simulation Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e79659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Variables for Simulation\n",
    "action_space_size = 8\n",
    "TRAFFIC_LIGHT_ID = \"traffic_light\"\n",
    "DELTA_PHASE_DURATION = 6 \n",
    "YELLOW_PHASE_DURATION = 4 \n",
    "lane_detectors = [f'q{i+1}' for i in range(8)]\n",
    "current_phase = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea609ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the queue length for each lane detector\n",
    "def get_queue_length():\n",
    "    return torch.tensor([\n",
    "        traci.lanearea.getLastStepHaltingNumber(d) for d in lane_detectors\n",
    "    ], dtype=torch.float)\n",
    "\n",
    "# The current state is defined as queue length for each lane detector\n",
    "def get_current_state():\n",
    "    return get_queue_length()\n",
    "\n",
    "# Simulate 20 seconds\n",
    "def simulate_time(seconds=1):\n",
    "    for _ in range(20 * seconds):\n",
    "        traci.simulationStep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6673eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(action):\n",
    "    global current_phase\n",
    "    if 2 * action == current_phase:\n",
    "        traci.trafficlight.setPhase(TRAFFIC_LIGHT_ID, 2 * action)\n",
    "        simulate_time(DELTA_PHASE_DURATION)\n",
    "    else:\n",
    "        traci.trafficlight.setPhase(TRAFFIC_LIGHT_ID, current_phase + 1)\n",
    "        simulate_time(YELLOW_PHASE_DURATION)\n",
    "        current_phase = 2 * action\n",
    "        traci.trafficlight.setPhase(TRAFFIC_LIGHT_ID, 2 * action)\n",
    "        simulate_time(DELTA_PHASE_DURATION)\n",
    "        \n",
    "    next_state = get_current_state()\n",
    "    reward = -torch.sum(next_state)\n",
    "    done = traci.simulation.getMinExpectedNumber() == 0\n",
    "    return next_state, reward, done, next_state.clone() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98af4b5b",
   "metadata": {},
   "source": [
    "### Q-Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af29ae5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_patterns = itertools.cycle([\"P1\", \"P2\", \"P3\", \"P4\"])\n",
    "\n",
    "def change_env():\n",
    "    pattern = next(traffic_patterns)\n",
    "    if traci.isLoaded():\n",
    "        traci.close()\n",
    "    traci.start(sumo_config(pattern))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f770667c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state, epsilon, policy_net):\n",
    "    if random.random() < epsilon:\n",
    "        return random.randint(0, action_space_size - 1)\n",
    "    else:\n",
    "        return torch.argmax(policy_net(state.unsqueeze(0))).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3243212c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimise_model(policy_net, target_net, memory, optimizer, batch_size, gamma):\n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "    batch = random.sample(memory, batch_size)\n",
    "    states = torch.stack([x[0] for x in batch])\n",
    "    actions = torch.tensor([x[1] for x in batch]).unsqueeze(1)\n",
    "    rewards = torch.tensor([x[2] for x in batch], dtype=torch.float)\n",
    "    next_states = torch.stack([x[3] for x in batch])\n",
    "    dones = torch.tensor([x[4] for x in batch], dtype=torch.float)\n",
    "\n",
    "    q_vals = policy_net(states).gather(1, actions).squeeze()\n",
    "    with torch.no_grad():\n",
    "        max_next_q_vals = target_net(next_states).max(1)[0]\n",
    "        target_vals = rewards + gamma * max_next_q_vals * (1 - dones)\n",
    "    loss = nn.MSELoss()(q_vals, target_vals)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15e9309",
   "metadata": {},
   "source": [
    "## **Baseline Model: Mimicing TrafficLightsNSW**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbeb9c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_baseline_results(episodes=40):\n",
    "    pattern_names = [\"P1\", \"P2\", \"P3\", \"P4\"]\n",
    "    pattern_name = itertools.cycle(pattern_names)\n",
    "    pattern_episode_count = collections.defaultdict(int)\n",
    "    pattern_rewards = collections.defaultdict(list)\n",
    "    pattern_avg_waits = collections.defaultdict(list)\n",
    "    pattern_max_waits = collections.defaultdict(list)\n",
    "    pattern_queue_lengths = {p: [] for p in pattern_names}\n",
    "\n",
    "    baseline_all_avg_queue_lengths = torch.zeros(len(lane_detectors), episodes)\n",
    "\n",
    "    phase_sequence = [2, 6, 4, 0]\n",
    "    green_duration = DELTA_PHASE_DURATION\n",
    "    yellow_duration = YELLOW_PHASE_DURATION\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        print(f\"[Baseline] Episode {episode}\")\n",
    "        current_pattern = next(pattern_name)\n",
    "        pattern_episode_count[current_pattern] += 1\n",
    "\n",
    "        change_env()\n",
    "\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        num_steps = 0\n",
    "\n",
    "        vehicle_wait_tracker = {}\n",
    "        queue_length_tracker = {}\n",
    "\n",
    "        state = get_current_state()\n",
    "        phase_index = 0\n",
    "\n",
    "        while not done:\n",
    "            green_phase = phase_sequence[phase_index]\n",
    "            yellow_phase = green_phase + 1\n",
    "\n",
    "            # Green\n",
    "            traci.trafficlight.setPhase(TRAFFIC_LIGHT_ID, green_phase)\n",
    "            simulate_time(green_duration)\n",
    "            num_steps += green_duration * 20\n",
    "\n",
    "            # Reward\n",
    "            state = get_current_state()\n",
    "            queue_size = torch.sum(state)\n",
    "            reward = -queue_size\n",
    "            episode_reward += reward\n",
    "\n",
    "            # Wait tracking\n",
    "            for v_id in traci.vehicle.getIDList():\n",
    "                wait_time = traci.vehicle.getWaitingTime(v_id)\n",
    "                if v_id not in vehicle_wait_tracker or wait_time > vehicle_wait_tracker[v_id]:\n",
    "                    vehicle_wait_tracker[v_id] = wait_time\n",
    "\n",
    "            # Queue length tracking\n",
    "            curr_queue = get_queue_length()\n",
    "            print(curr_queue)\n",
    "            for i in range(len(curr_queue)):\n",
    "                queue_length_tracker[i] = queue_length_tracker.get(i, 0) + curr_queue[i]\n",
    "\n",
    "            done = traci.simulation.getMinExpectedNumber() == 0\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            # Yellow\n",
    "            traci.trafficlight.setPhase(TRAFFIC_LIGHT_ID, yellow_phase)\n",
    "            simulate_time(yellow_duration)\n",
    "            num_steps += yellow_duration * 20\n",
    "\n",
    "            done = traci.simulation.getMinExpectedNumber() == 0\n",
    "            phase_index = (phase_index + 1) % len(phase_sequence)\n",
    "\n",
    "        print(\"Steps per episode:\", num_steps)\n",
    "\n",
    "        avg_queue_lengths = [queue_length_tracker.get(i, 0.0) / num_steps for i in range(len(lane_detectors))]\n",
    "        pattern_queue_lengths[current_pattern].append(avg_queue_lengths)\n",
    "\n",
    "        vehicle_waits = list(vehicle_wait_tracker.values())\n",
    "        avg_wait = sum(vehicle_waits) / len(vehicle_waits) if vehicle_waits else 0.0\n",
    "        max_wait = max(vehicle_waits) if vehicle_waits else 0.0\n",
    "\n",
    "        pattern_rewards[current_pattern].append(episode_reward)\n",
    "        pattern_avg_waits[current_pattern].append(avg_wait)\n",
    "        pattern_max_waits[current_pattern].append(max_wait)\n",
    "\n",
    "        print(torch.sum(baseline_all_avg_queue_lengths[:, episode]))\n",
    "\n",
    "    for pattern in pattern_names:\n",
    "        print(f\"\\nPattern {pattern}:\")\n",
    "        print(f\"  Episodes: {pattern_episode_count[pattern]}\")\n",
    "        print(f\"  Avg Reward: {sum(pattern_rewards[pattern]) / len(pattern_rewards[pattern]):.2f}\")\n",
    "        print(f\"  Avg Wait: {sum(pattern_avg_waits[pattern]) / len(pattern_avg_waits[pattern]):.2f}\")\n",
    "        print(f\"  Max Wait: {max(pattern_max_waits[pattern]):.2f}\")\n",
    "\n",
    "    return (\n",
    "        pattern_rewards,\n",
    "        pattern_avg_waits,\n",
    "        pattern_max_waits,\n",
    "        pattern_queue_lengths\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a63353e",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_results = get_baseline_results(episodes=40)\n",
    "baseline_rewards, baseline_avg_waits, baseline_max_waits, baseline_queues = baseline_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66de2c47",
   "metadata": {},
   "source": [
    "## **Architecture 1: Fully Connected Q-Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08580c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 64), nn.ReLU(),\n",
    "            nn.Linear(64, action_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8674c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The current state is defined as queue length for each lane detector\n",
    "def get_current_state():\n",
    "    return get_queue_length()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a10b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_algorithm(params, episodes=200):\n",
    "    gamma = params[\"gamma\"]\n",
    "    epsilon = params[\"epsilon\"]\n",
    "    epsilon_decay = params[\"epsilon_decay\"]\n",
    "    min_epsilon = params[\"min_epsilon\"]\n",
    "    lr = params[\"learning_rate\"]\n",
    "    batch_size = params[\"batch_size\"]\n",
    "    target_update_freq = params[\"target_update_freq\"]\n",
    "    memory_size = params[\"memory_size\"]\n",
    "\n",
    "    state_dim = len(lane_detectors)\n",
    "    policy_net = DQN(state_dim, action_space_size)\n",
    "    target_net = DQN(state_dim, action_space_size)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=lr)\n",
    "    memory = deque(maxlen=memory_size)\n",
    "\n",
    "    rewards_per_episode = []\n",
    "    steps_done = 0\n",
    "\n",
    "    avg_wait_per_ep = []\n",
    "    max_wait_per_ep = []\n",
    "    all_avg_queue_lengths = torch.zeros(len(lane_detectors), episodes)\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        change_env()\n",
    "        state = get_current_state()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "\n",
    "        vehicle_wait_tracker = {}\n",
    "        queue_length_tracker = {}\n",
    "        num_steps = 0\n",
    "\n",
    "        while not done:\n",
    "            action = choose_action(state, epsilon, policy_net)\n",
    "            next_state, reward, done, curr_queue = step(action)\n",
    "            memory.append((state, action, reward, next_state, done))\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "\n",
    "            # Vehicle wait time tracking\n",
    "            for v_id in traci.vehicle.getIDList():\n",
    "                wait_time = traci.vehicle.getWaitingTime(v_id)\n",
    "                if v_id not in vehicle_wait_tracker:\n",
    "                    vehicle_wait_tracker[v_id] = wait_time\n",
    "                elif wait_time > vehicle_wait_tracker[v_id]:\n",
    "                    vehicle_wait_tracker[v_id] = wait_time\n",
    "\n",
    "            # Queue length tracking\n",
    "            for i in range(len(curr_queue)):\n",
    "                queue_length_tracker[i] = queue_length_tracker.get(i, 0) + curr_queue[i]\n",
    "\n",
    "            optimise_model(policy_net, target_net, memory, optimizer, batch_size, gamma)\n",
    "            if steps_done % target_update_freq == 0:\n",
    "                target_net.load_state_dict(policy_net.state_dict())\n",
    "            steps_done += 1\n",
    "            num_steps += 1\n",
    "\n",
    "        # Aggregate queue stats\n",
    "        for i, total_len in queue_length_tracker.items():\n",
    "            all_avg_queue_lengths[i, episode] = total_len / num_steps\n",
    "\n",
    "        vehicle_waits = list(vehicle_wait_tracker.values())\n",
    "        avg_wait = sum(vehicle_waits) / len(vehicle_waits) if vehicle_waits else 0.0\n",
    "        max_wait = max(vehicle_waits) if vehicle_waits else 0.0\n",
    "        avg_wait_per_ep.append(avg_wait)\n",
    "        max_wait_per_ep.append(max_wait)\n",
    "\n",
    "        epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "        rewards_per_episode.append(episode_reward.item())\n",
    "\n",
    "    N = max(1, int(0.2 * episodes))  # 20% of episodes, at least 1\n",
    "    avg_reward_last_N = sum(rewards_per_episode[-N:]) / N\n",
    "    avg_wait_last_N = sum(avg_wait_per_ep[-N:]) / N\n",
    "    max_wait_last_N = max(max_wait_per_ep[-N:]) if max_wait_per_ep[-N:] else 0.0\n",
    "\n",
    "    return {\n",
    "        \"avg_reward\": sum(rewards_per_episode) / episodes,\n",
    "        \"avg_wait_per_ep\": avg_wait_per_ep,\n",
    "        \"max_wait_per_ep\": max_wait_per_ep,\n",
    "        \"avg_queue_lengths\": all_avg_queue_lengths,\n",
    "        \"rewards_per_episode\": rewards_per_episode,\n",
    "        \"avg_reward_last_N\": avg_reward_last_N,\n",
    "        \"avg_wait_last_N\": avg_wait_last_N,\n",
    "        \"max_wait_last_N\": max_wait_last_N,\n",
    "        \"trained_model\": policy_net\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae015cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_params = {\n",
    "    \"gamma\": 0.99,\n",
    "    \"epsilon\": 0.9,\n",
    "    \"epsilon_decay\": 0.95,\n",
    "    \"min_epsilon\": 0.05,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"batch_size\": 128,\n",
    "    \"target_update_freq\": 1800,\n",
    "    \"memory_size\": 20000\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a88624",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_results = train_algorithm(optimal_params, episodes=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050618a6",
   "metadata": {},
   "source": [
    "## **Architecture 2: CNN Q-Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee5a0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, num_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=4, out_channels=32, kernel_size=8, stride=4)  \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)  \n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)  \n",
    "        \n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 512)\n",
    "        self.output = nn.Linear(512, num_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x / 255.0  \n",
    "        x = F.relu(self.conv1(x))   \n",
    "        x = F.relu(self.conv2(x))  \n",
    "        x = F.relu(self.conv3(x))   \n",
    "        \n",
    "        x = x.view(x.size(0), -1)   \n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.output(x)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1746bee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_occupancy_grid(grid_size=(84, 84),\n",
    "                             bounds=(-91.5, 76.5, -66.5, 101.5)):\n",
    "    x_min, x_max, y_min, y_max = bounds\n",
    "    x_scale = grid_size[1] / (x_max - x_min)\n",
    "    y_scale = grid_size[0] / (y_max - y_min)\n",
    "\n",
    "    grid = torch.zeros(grid_size, dtype=torch.float32)\n",
    "\n",
    "    for v_id in traci.vehicle.getIDList():\n",
    "        x, y = traci.vehicle.getPosition(v_id)\n",
    "\n",
    "        if x_min <= x <= x_max and y_min <= y <= y_max:\n",
    "            col = int((x - x_min) * x_scale)\n",
    "            row = int((y - y_min) * y_scale)\n",
    "\n",
    "            if 0 <= row < grid_size[0] and 0 <= col < grid_size[1]:\n",
    "                grid[row, col] = 1.0  \n",
    "\n",
    "    return grid\n",
    "\n",
    "frame_buffer = deque(maxlen=4)\n",
    "\n",
    "def get_current_state():\n",
    "    frame_buffer.clear()  # clear previous frames to maintain consistency\n",
    "\n",
    "    for _ in range(4):\n",
    "        simulate_time(1)  # simulate 1 second\n",
    "        grid = generate_occupancy_grid()\n",
    "        frame_buffer.append(grid.unsqueeze(0))\n",
    "\n",
    "    state = torch.cat(list(frame_buffer), dim=0)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de08cc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_algorithm(params, episodes=1):\n",
    "    gamma = params[\"gamma\"]\n",
    "    epsilon = params[\"epsilon\"]\n",
    "    epsilon_decay = params[\"epsilon_decay\"]\n",
    "    min_epsilon = params[\"min_epsilon\"]\n",
    "    lr = params[\"learning_rate\"]\n",
    "    batch_size = params[\"batch_size\"]\n",
    "    target_update_freq = params[\"target_update_freq\"]\n",
    "    memory_size = params[\"memory_size\"]\n",
    "\n",
    "    policy_net = DQN(action_space_size)\n",
    "    target_net = DQN(action_space_size)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=lr)\n",
    "    memory = deque(maxlen=memory_size)\n",
    "\n",
    "    rewards_per_episode = []\n",
    "    steps_done = 0\n",
    "\n",
    "    avg_wait_per_ep = []\n",
    "    max_wait_per_ep = []\n",
    "    all_avg_queue_lengths = torch.zeros(len(lane_detectors), episodes)\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        change_env()\n",
    "        state = get_current_state()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "\n",
    "        vehicle_wait_tracker = {}\n",
    "        queue_length_tracker = {}\n",
    "        num_steps = 0\n",
    "\n",
    "        while not done:\n",
    "            action = choose_action(state, epsilon, policy_net)\n",
    "            next_state, reward, done, curr_queue = step(action)\n",
    "            memory.append((state, action, reward, next_state, done))\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "\n",
    "            # Vehicle wait time tracking\n",
    "            for v_id in traci.vehicle.getIDList():\n",
    "                wait_time = traci.vehicle.getWaitingTime(v_id)\n",
    "                if v_id not in vehicle_wait_tracker:\n",
    "                    vehicle_wait_tracker[v_id] = wait_time\n",
    "                elif wait_time > vehicle_wait_tracker[v_id]:\n",
    "                    vehicle_wait_tracker[v_id] = wait_time\n",
    "\n",
    "            # Queue length tracking\n",
    "            for i in range(len(curr_queue)):\n",
    "                queue_length_tracker[i] = queue_length_tracker.get(i, 0) + curr_queue[i]\n",
    "\n",
    "            optimise_model(policy_net, target_net, memory, optimizer, batch_size, gamma)\n",
    "            if steps_done % target_update_freq == 0:\n",
    "                target_net.load_state_dict(policy_net.state_dict())\n",
    "            steps_done += 1\n",
    "            num_steps += 1\n",
    "\n",
    "        # Aggregate queue stats\n",
    "        for i, total_len in queue_length_tracker.items():\n",
    "            all_avg_queue_lengths[i, episode] = total_len / num_steps\n",
    "\n",
    "        vehicle_waits = list(vehicle_wait_tracker.values())\n",
    "        avg_wait = sum(vehicle_waits) / len(vehicle_waits) if vehicle_waits else 0.0\n",
    "        max_wait = max(vehicle_waits) if vehicle_waits else 0.0\n",
    "        avg_wait_per_ep.append(avg_wait)\n",
    "        max_wait_per_ep.append(max_wait)\n",
    "\n",
    "        epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "        rewards_per_episode.append(episode_reward.item())\n",
    "        print(episode_reward)\n",
    "\n",
    "    N = max(1, int(0.2 * episodes))  # 20% of episodes, at least 1\n",
    "    avg_reward_last_N = sum(rewards_per_episode[-N:]) / N\n",
    "    avg_wait_last_N = sum(avg_wait_per_ep[-N:]) / N\n",
    "    max_wait_last_N = max(max_wait_per_ep[-N:]) if max_wait_per_ep[-N:] else 0.0\n",
    "\n",
    "    return {\n",
    "        \"avg_reward\": sum(rewards_per_episode) / episodes,\n",
    "        \"avg_wait_per_ep\": avg_wait_per_ep,\n",
    "        \"max_wait_per_ep\": max_wait_per_ep,\n",
    "        \"avg_queue_lengths\": all_avg_queue_lengths,\n",
    "        \"rewards_per_episode\": rewards_per_episode,\n",
    "        \"avg_reward_last_N\": avg_reward_last_N,\n",
    "        \"avg_wait_last_N\": avg_wait_last_N,\n",
    "        \"max_wait_last_N\": max_wait_last_N,\n",
    "        \"trained_model\": policy_net\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b65a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_params = {\n",
    "    \"gamma\": 0.999,\n",
    "    \"epsilon\": 0.9,\n",
    "    \"epsilon_decay\": 0.95,\n",
    "    \"min_epsilon\": 0.05,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"batch_size\": 128,\n",
    "    \"target_update_freq\": 1800,\n",
    "    \"memory_size\": 20000\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96689d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_results = train_algorithm(optimal_params, episodes=200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
