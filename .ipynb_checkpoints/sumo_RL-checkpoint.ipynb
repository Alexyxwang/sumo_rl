{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc9b5a33-0847-49f4-bd26-e112fe4474de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23fad82c-c5f9-486a-982d-c3f022f15f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"SUMO_HOME\" in os.environ:\n",
    "    tools = os.path.join(os.environ[\"SUMO_HOME\"], \"tools\")\n",
    "    sys.path.append(tools)\n",
    "else:\n",
    "    sys.exit(\"Please declare environment variable 'SUMO_HOME'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35d8d44e-4be3-4230-9b84-9a8ff12a8c26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22, 'SUMO 1.23.1')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import traci\n",
    "\n",
    "def sumo_config(traffic_pattern=\"junction\"):\n",
    "    sumo_config = [\n",
    "        \"sumo\",\n",
    "        \"-c\", \"SUMO_networks/\" + traffic_pattern + \".sumocfg\",\n",
    "        \"--step-length\", \"0.05\",\n",
    "        \"--delay\", \"0\",\n",
    "        \"--lateral-resolution\", \"0.1\",\n",
    "        \"--start\",\n",
    "        \"--no-warnings\",\n",
    "        \"--no-step-log\",\n",
    "    ]\n",
    "    return sumo_config\n",
    "\n",
    "traci.start(sumo_config())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e510631a-32ac-4309-a828-f32081b3bb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = torch.tensor([0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.float)\n",
    "action_space_size = 8\n",
    "TRAFFIC_LIGHT_ID = \"traffic_light\"\n",
    "DELTA_PHASE_DURATION = 6\n",
    "YELLOW_PHASE_DURATION = 4\n",
    "lane_detectors = ['q1', 'q2', 'q3', 'q4', 'q5', 'q6', 'q7', 'q8']\n",
    "action = []\n",
    "current_phase = 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28adf800-f9d0-4668-8879-e46eb36aa112",
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_patterns = [\"P1\", \"P2\", \"P3\", \"P4\"]\n",
    "def change_env():\n",
    "    traci.close()\n",
    "    traci.start(sumo_config())\n",
    "\n",
    "def get_current_state():\n",
    "    state = []\n",
    "    for detector in lane_detectors:\n",
    "        state.append(traci.lanearea.getLastStepHaltingNumber(detector))\n",
    "    return torch.tensor(state, dtype=torch.float)\n",
    "\n",
    "def simulate_time(seconds = 1):\n",
    "    for i in range(20 * seconds):\n",
    "        traci.simulationStep()\n",
    "\n",
    "def step(action):\n",
    "    global current_phase\n",
    "\n",
    "    if 2 * action == current_phase:\n",
    "        traci.trafficlight.setPhase(TRAFFIC_LIGHT_ID, 2 * action)\n",
    "        simulate_time(DELTA_PHASE_DURATION)\n",
    "        next_state = get_current_state()\n",
    "        next_queue_size = torch.sum(next_state)\n",
    "        reward =  -next_queue_size\n",
    "        done = traci.simulation.getMinExpectedNumber() == 0\n",
    "        return next_state, reward, done\n",
    "    else:\n",
    "        traci.trafficlight.setPhase(TRAFFIC_LIGHT_ID, current_phase + 1)\n",
    "        simulate_time(YELLOW_PHASE_DURATION)\n",
    "        current_phase = 2 * action\n",
    "        traci.trafficlight.setPhase(TRAFFIC_LIGHT_ID, 2 * action)\n",
    "        simulate_time(DELTA_PHASE_DURATION)\n",
    "        next_state = get_current_state()\n",
    "        next_queue_size = torch.sum(next_state)\n",
    "        reward =  -next_queue_size\n",
    "        done = traci.simulation.getMinExpectedNumber() == 0\n",
    "        return next_state, reward, done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8461fbb2-cef4-4668-ae1d-194af81a1b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_space_size, action_space_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(state_space_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, action_space_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.main(x)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62109aab-25c4-4758-a907-12b26dea15bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "# discount factor\n",
    "gamma = 0.999\n",
    "\n",
    "# starting exploration rate\n",
    "epsilon = 0.9\n",
    "\n",
    "# rate of decay as model becomes more stable (5%)\n",
    "epsilon_decay = 0.95\n",
    "\n",
    "# final exploration rate for stable model\n",
    "min_epsilon = 0.05\n",
    "batch_size = 128\n",
    "target_update_freq = 400\n",
    "memory_size = 10000\n",
    "episodes = 75\n",
    "# episodes = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82afad9d-70fa-41d4-bd57-128034dca176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters to tune:\n",
    "TEMP_GARBO_VAR = 100000000000\n",
    "\n",
    "\n",
    "# discount factor\n",
    "gamma = [0.999, TEMP_GARBO_VAR]\n",
    "# starting exploration rate\n",
    "epsilon = [0.9, TEMP_GARBO_VAR]\n",
    "# rate of decay as model becomes more stable (5%)\n",
    "epsilon_decay = [0.95, TEMP_GARBO_VAR]\n",
    "# final exploration rate for stable model\n",
    "min_epsilon = [0.05, TEMP_GARBO_VAR]\n",
    "\n",
    "# NN related parameters\n",
    "learning_rate = [0.01, TEMP_GARBO_VAR]\n",
    "batch_size = [128, TEMP_GARBO_VAR]\n",
    "target_update_freq = [400, TEMP_GARBO_VAR]\n",
    "memory_size = [10000, TEMP_GARBO_VAR]\n",
    "\n",
    "# doen think we need to both tuning episodes\n",
    "episodes = 150\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "param_combinations = list(product(\n",
    "    gamma,\n",
    "    epsilon,\n",
    "    epsilon_decay,\n",
    "    min_epsilon,\n",
    "    learning_rate,\n",
    "    batch_size,\n",
    "    target_update_freq,\n",
    "    memory_size\n",
    "))\n",
    "\n",
    "# Example usage below\n",
    "\n",
    "# for combination in param_combinations:\n",
    "#     g, e, e_decay, min_e, lr, vs, target_freq, mem_size = combination\n",
    "#     ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8ac4816-82fd-41bb-bef2-642157aea0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "policy_net = DQN(state.shape[0], action_space_size)\n",
    "\n",
    "target_net = DQN(state.shape[0], action_space_size)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "memory = deque(maxlen=memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ce62be0-55a2-4ec5-87d1-bed63044c10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def choose_action(state, epsilon):\n",
    "    if random.random() < epsilon:\n",
    "        return random.randint(0, action_space_size - 1)\n",
    "    else:\n",
    "        q_values = policy_net(state.unsqueeze(0))\n",
    "        return torch.argmax(q_values).item()\n",
    "\n",
    "\n",
    "def optimise_model():\n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "\n",
    "    batch = random.sample(memory, batch_size)\n",
    "\n",
    "    state_batch = torch.stack([b[0] for b in batch]).float()\n",
    "    next_state_batch = torch.stack([b[3] for b in batch]).float()\n",
    "    reward_batch = torch.tensor([b[2] for b in batch], dtype=torch.float)\n",
    "    action_batch = torch.tensor([b[1] for b in batch], dtype=torch.long).unsqueeze(1)\n",
    "    done_batch = torch.tensor([b[4] for b in batch], dtype=torch.float)\n",
    "\n",
    "    q_values = policy_net(state_batch).gather(1, action_batch).squeeze()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        max_next_q_values = target_net(next_state_batch).max(1)[0]\n",
    "        target_q_values = reward_batch + gamma * max_next_q_values * (1 - done_batch)\n",
    "\n",
    "    loss = nn.MSELoss()(q_values, target_q_values)\n",
    "\n",
    "    # print(f\"Training loss: {loss.item()}\")\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89e2156a-f7f8-4ea2-bfa3-aea61128000a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_algorithm(episodes, epsilon, epsilon_decay, min_epsilon): #add parameters here later\n",
    "    rewards_per_episode = []\n",
    "    steps_done = 0\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        print(f\"Episode {episode}\")\n",
    "        change_env()\n",
    "    \n",
    "        state = get_current_state()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "    \n",
    "        while not done:\n",
    "            # Select action\n",
    "            action = choose_action(state, epsilon)\n",
    "            next_state, reward, done = step(action)\n",
    "    \n",
    "            # print(f\"Action={action}, Reward={reward:.2f}, Done={done}\")\n",
    "            # print(f\"Next State: {next_state.tolist()}\")\n",
    "            # Store transition in memory\n",
    "            memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "            # Update state\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            # print(episode_reward)\n",
    "            # Optimize model\n",
    "            optimise_model()\n",
    "    \n",
    "            # Update target network periodically\n",
    "            if steps_done % target_update_freq == 0:\n",
    "                target_net.load_state_dict(policy_net.state_dict())\n",
    "    \n",
    "            steps_done += 1\n",
    "    \n",
    "        # Decay epsilon\n",
    "        epsilon = max(min_epsilon, epsilon_decay * epsilon)\n",
    "        print(episode_reward)\n",
    "        rewards_per_episode.append(episode_reward)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51bf513d-c28b-461f-9a23-26d9eef8115e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\n",
      "tensor(-6526.)\n",
      "Episode 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrain_algorithm\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon_decay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_epsilon\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mtrain_algorithm\u001b[39m\u001b[34m(episodes, epsilon, epsilon_decay, min_epsilon)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[32m     14\u001b[39m     \u001b[38;5;66;03m# Select action\u001b[39;00m\n\u001b[32m     15\u001b[39m     action = choose_action(state, epsilon)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     next_state, reward, done = \u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m     \u001b[38;5;66;03m# print(f\"Action={action}, Reward={reward:.2f}, Done={done}\")\u001b[39;00m\n\u001b[32m     19\u001b[39m     \u001b[38;5;66;03m# print(f\"Next State: {next_state.tolist()}\")\u001b[39;00m\n\u001b[32m     20\u001b[39m     \u001b[38;5;66;03m# Store transition in memory\u001b[39;00m\n\u001b[32m     21\u001b[39m     memory.append((state, action, reward, next_state, done))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mstep\u001b[39m\u001b[34m(action)\u001b[39m\n\u001b[32m     30\u001b[39m current_phase = \u001b[32m2\u001b[39m * action\n\u001b[32m     31\u001b[39m traci.trafficlight.setPhase(TRAFFIC_LIGHT_ID, \u001b[32m2\u001b[39m * action)\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[43msimulate_time\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDELTA_PHASE_DURATION\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m next_state = get_current_state()\n\u001b[32m     34\u001b[39m next_queue_size = torch.sum(next_state)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36msimulate_time\u001b[39m\u001b[34m(seconds)\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msimulate_time\u001b[39m(seconds = \u001b[32m1\u001b[39m):\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m20\u001b[39m * seconds):\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m         \u001b[43mtraci\u001b[49m\u001b[43m.\u001b[49m\u001b[43msimulationStep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\main.py:200\u001b[39m, in \u001b[36msimulationStep\u001b[39m\u001b[34m(step)\u001b[39m\n\u001b[32m    194\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msimulationStep\u001b[39m(step=\u001b[32m0\u001b[39m):\n\u001b[32m    195\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"simulationStep(float) -> None\u001b[39;00m\n\u001b[32m    196\u001b[39m \u001b[33;03m    Make a simulation step and simulate up to the given second in sim time.\u001b[39;00m\n\u001b[32m    197\u001b[39m \u001b[33;03m    If the given value is 0 or absent, exactly one step is performed.\u001b[39;00m\n\u001b[32m    198\u001b[39m \u001b[33;03m    Values smaller than or equal to the current sim time result in no action.\u001b[39;00m\n\u001b[32m    199\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m     \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msimulationStep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\connection.py:370\u001b[39m, in \u001b[36mConnection.simulationStep\u001b[39m\u001b[34m(self, step)\u001b[39m\n\u001b[32m    368\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(step) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mint\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m step >= \u001b[32m1000\u001b[39m:\n\u001b[32m    369\u001b[39m     warnings.warn(\u001b[33m\"\u001b[39m\u001b[33mAPI change now handles step as floating point seconds\u001b[39m\u001b[33m\"\u001b[39m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m370\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sendCmd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCMD_SIMSTEP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mD\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m subscriptionResults \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._subscriptionMapping.values():\n\u001b[32m    372\u001b[39m     subscriptionResults.reset()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\connection.py:232\u001b[39m, in \u001b[36mConnection._sendCmd\u001b[39m\u001b[34m(self, cmdID, varID, objID, format, *values)\u001b[39m\n\u001b[32m    230\u001b[39m     \u001b[38;5;28mself\u001b[39m._string += struct.pack(\u001b[33m\"\u001b[39m\u001b[33m!i\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(objID)) + objID\n\u001b[32m    231\u001b[39m \u001b[38;5;28mself\u001b[39m._string += packed\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sendExact\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\connection.py:131\u001b[39m, in \u001b[36mConnection._sendExact\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    129\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33msending\u001b[39m\u001b[33m\"\u001b[39m, Storage(length + \u001b[38;5;28mself\u001b[39m._string).getDebugString())\n\u001b[32m    130\u001b[39m \u001b[38;5;28mself\u001b[39m._socket.send(length + \u001b[38;5;28mself\u001b[39m._string)\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_recvExact\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _DEBUG:\n\u001b[32m    133\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mreceiving\u001b[39m\u001b[33m\"\u001b[39m, result.getDebugString())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\connection.py:109\u001b[39m, in \u001b[36mConnection._recvExact\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    107\u001b[39m result = \u001b[38;5;28mbytes\u001b[39m()\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result) < \u001b[32m4\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m     t = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_socket\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    110\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t:\n\u001b[32m    111\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "train_algorithm(10, epsilon, epsilon_decay, min_epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8ad3db-b0af-43e9-8508-2a6c158235c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy_net.state_dict(), \"dqn_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f59140-1f51-493b-af96-b0a708ea333e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(rewards_per_episode)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.title(\"DQN on traffic lights\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
