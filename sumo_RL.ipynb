{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc9b5a33-0847-49f4-bd26-e112fe4474de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23fad82c-c5f9-486a-982d-c3f022f15f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"SUMO_HOME\" in os.environ:\n",
    "    tools = os.path.join(os.environ[\"SUMO_HOME\"], \"tools\")\n",
    "    sys.path.append(tools)\n",
    "else:\n",
    "    sys.exit(\"Please declare environment variable 'SUMO_HOME'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35d8d44e-4be3-4230-9b84-9a8ff12a8c26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22, 'SUMO 1.23.1')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import traci\n",
    "\n",
    "def sumo_config(traffic_pattern=\"P1\"):\n",
    "    sumo_config = [\n",
    "        \"sumo\",\n",
    "        \"-c\", \"SUMO_networks/\" + traffic_pattern + \"/junction.sumocfg\",\n",
    "        \"--step-length\", \"0.05\",\n",
    "        \"--delay\", \"0\",\n",
    "        \"--lateral-resolution\", \"0.1\",\n",
    "        \"--start\",\n",
    "        \"--no-warnings\",\n",
    "        \"--no-step-log\",\n",
    "    ]\n",
    "    return sumo_config\n",
    "\n",
    "\n",
    "\n",
    "traci.start(sumo_config())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e510631a-32ac-4309-a828-f32081b3bb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = torch.tensor([0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.float)\n",
    "action_space_size = 8\n",
    "TRAFFIC_LIGHT_ID = \"traffic_light\"\n",
    "DELTA_PHASE_DURATION = 6\n",
    "YELLOW_PHASE_DURATION = 4\n",
    "lane_detectors = ['q1', 'q2', 'q3', 'q4', 'q5', 'q6', 'q7', 'q8']\n",
    "action = []\n",
    "current_phase = 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28adf800-f9d0-4668-8879-e46eb36aa112",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "traffic_patterns = [\"P1\", \"P2\", \"P3\", \"P4\"]\n",
    "traffic_pattern = itertools.cycle(traffic_patterns)\n",
    "\n",
    "def change_env():\n",
    "    pattern = next(traffic_pattern)  \n",
    "    traci.close()\n",
    "    traci.start(sumo_config(pattern))\n",
    "\n",
    "\n",
    "def get_queue_length():\n",
    "    state = []\n",
    "    for detector in lane_detectors:\n",
    "        state.append(traci.lanearea.getLastStepHaltingNumber(detector))\n",
    "    return torch.tensor(state, dtype=torch.float)\n",
    "\n",
    "def get_current_state():\n",
    "    return get_queue_length()\n",
    "\n",
    "def simulate_time(seconds = 1):\n",
    "    for i in range(20 * seconds):\n",
    "        traci.simulationStep()\n",
    "\n",
    "def step(action):\n",
    "    global current_phase\n",
    "\n",
    "    if 2 * action == current_phase:\n",
    "        traci.trafficlight.setPhase(TRAFFIC_LIGHT_ID, 2 * action)\n",
    "        simulate_time(DELTA_PHASE_DURATION)\n",
    "        next_state = get_current_state()\n",
    "        next_queue_size = torch.sum(next_state)\n",
    "        reward =  -next_queue_size\n",
    "        done = traci.simulation.getMinExpectedNumber() == 0\n",
    "        return next_state, reward, done\n",
    "    else:\n",
    "        traci.trafficlight.setPhase(TRAFFIC_LIGHT_ID, current_phase + 1)\n",
    "        simulate_time(YELLOW_PHASE_DURATION)\n",
    "        current_phase = 2 * action\n",
    "        traci.trafficlight.setPhase(TRAFFIC_LIGHT_ID, 2 * action)\n",
    "        simulate_time(DELTA_PHASE_DURATION)\n",
    "        next_state = get_current_state()\n",
    "        next_queue_size = torch.sum(next_state)\n",
    "        reward =  -next_queue_size\n",
    "        done = traci.simulation.getMinExpectedNumber() == 0\n",
    "        return next_state, reward, done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8461fbb2-cef4-4668-ae1d-194af81a1b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_space_size, action_space_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(state_space_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, action_space_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.main(x)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62109aab-25c4-4758-a907-12b26dea15bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "# discount factor\n",
    "gamma = 0.999\n",
    "\n",
    "# starting exploration rate\n",
    "epsilon = 0.9\n",
    "\n",
    "# rate of decay as model becomes more stable (5%)\n",
    "epsilon_decay = 0.95\n",
    "\n",
    "# final exploration rate for stable model\n",
    "min_epsilon = 0.05\n",
    "batch_size = 128\n",
    "target_update_freq = 400\n",
    "memory_size = 10000\n",
    "episodes = 75\n",
    "# episodes = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82afad9d-70fa-41d4-bd57-128034dca176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define hyperparameters to tune:\n",
    "# TEMP_GARBO_VAR = 100000000000\n",
    "\n",
    "\n",
    "# # discount factor\n",
    "# gamma = [0.999, TEMP_GARBO_VAR]\n",
    "# # starting exploration rate\n",
    "# epsilon = [0.9, TEMP_GARBO_VAR]\n",
    "# # rate of decay as model becomes more stable (5%)\n",
    "# epsilon_decay = [0.95, TEMP_GARBO_VAR]\n",
    "# # final exploration rate for stable model\n",
    "# min_epsilon = [0.05, TEMP_GARBO_VAR]\n",
    "\n",
    "# # NN related parameters\n",
    "# learning_rate = [0.01, TEMP_GARBO_VAR]\n",
    "# batch_size = [128, TEMP_GARBO_VAR]\n",
    "# target_update_freq = [400, TEMP_GARBO_VAR]\n",
    "# memory_size = [10000, TEMP_GARBO_VAR]\n",
    "\n",
    "# # doen think we need to both tuning episodes\n",
    "# episodes = 150\n",
    "\n",
    "# from itertools import product\n",
    "\n",
    "# param_combinations = list(product(\n",
    "#     gamma,\n",
    "#     epsilon,\n",
    "#     epsilon_decay,\n",
    "#     min_epsilon,\n",
    "#     learning_rate,\n",
    "#     batch_size,\n",
    "#     target_update_freq,\n",
    "#     memory_size\n",
    "# ))\n",
    "\n",
    "# Example usage below\n",
    "\n",
    "# for combination in param_combinations:\n",
    "#     g, e, e_decay, min_e, lr, vs, target_freq, mem_size = combination\n",
    "#     ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8ac4816-82fd-41bb-bef2-642157aea0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "policy_net = DQN(state.shape[0], action_space_size)\n",
    "\n",
    "target_net = DQN(state.shape[0], action_space_size)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "memory = deque(maxlen=memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ce62be0-55a2-4ec5-87d1-bed63044c10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def choose_action(state, epsilon):\n",
    "    if random.random() < epsilon:\n",
    "        return random.randint(0, action_space_size - 1)\n",
    "    else:\n",
    "        q_values = policy_net(state.unsqueeze(0))\n",
    "        return torch.argmax(q_values).item()\n",
    "\n",
    "\n",
    "def optimise_model():\n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "\n",
    "    batch = random.sample(memory, batch_size)\n",
    "\n",
    "    state_batch = torch.stack([b[0] for b in batch]).float()\n",
    "    next_state_batch = torch.stack([b[3] for b in batch]).float()\n",
    "    reward_batch = torch.tensor([b[2] for b in batch], dtype=torch.float)\n",
    "    action_batch = torch.tensor([b[1] for b in batch], dtype=torch.long).unsqueeze(1)\n",
    "    done_batch = torch.tensor([b[4] for b in batch], dtype=torch.float)\n",
    "\n",
    "    q_values = policy_net(state_batch).gather(1, action_batch).squeeze()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        max_next_q_values = target_net(next_state_batch).max(1)[0]\n",
    "        target_q_values = reward_batch + gamma * max_next_q_values * (1 - done_batch)\n",
    "\n",
    "    loss = nn.MSELoss()(q_values, target_q_values)\n",
    "\n",
    "    # print(f\"Training loss: {loss.item()}\")\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89e2156a-f7f8-4ea2-bfa3-aea61128000a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_algorithm(episodes, epsilon, epsilon_decay, min_epsilon): #add parameters here later\n",
    "    rewards_per_episode = []\n",
    "    avg_wait_per_ep = []\n",
    "    max_wait_per_ep = []\n",
    "    \n",
    "    num_lanes = len(lane_detectors)\n",
    "    all_avg_queue_lengths = torch.zeros(num_lanes, episodes)\n",
    "    \n",
    "    steps_done = 0\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        print(f\"Episode {episode}\")\n",
    "        change_env()\n",
    "    \n",
    "        state = get_current_state()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "\n",
    "        vehicle_wait_tracker = {} # NEW\n",
    "        queue_length_tracker = {}\n",
    "        num_steps = 0\n",
    "        while not done:\n",
    "            num_steps += 1 \n",
    "            # Select action\n",
    "            action = choose_action(state, epsilon)\n",
    "            next_state, reward, done = step(action)\n",
    "\n",
    "\n",
    "            # ADDITIONS\n",
    "            for v_id in traci.vehicle.getIDList():\n",
    "                wait_time = traci.vehicle.getWaitingTime(v_id)\n",
    "    \n",
    "                if v_id not in vehicle_wait_tracker:\n",
    "                    vehicle_wait_tracker[v_id] = wait_time\n",
    "                elif wait_time > vehicle_wait_tracker[v_id]:\n",
    "                    vehicle_wait_tracker[v_id] = wait_time\n",
    "            #\n",
    "    \n",
    "            # retrive queue length: list of size 8 (one number for each lane detector)\n",
    "            curr_queue = get_queue_length()\n",
    "            for i in range(len(curr_queue)):\n",
    "                if i not in queue_length_tracker:\n",
    "                    queue_length_tracker[i] = curr_queue[i]\n",
    "                else:\n",
    "                    queue_length_tracker[i] += curr_queue[i]\n",
    "    \n",
    "                \n",
    "            # print(f\"Action={action}, Reward={reward:.2f}, Done={done}\")\n",
    "            # print(f\"Next State: {next_state.tolist()}\")\n",
    "            # Store transition in memory\n",
    "            memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "            # Update state\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            # print(episode_reward)\n",
    "            # Optimize model\n",
    "            optimise_model()\n",
    "    \n",
    "            # Update target network periodically\n",
    "            if steps_done % target_update_freq == 0:\n",
    "                target_net.load_state_dict(policy_net.state_dict())\n",
    "    \n",
    "            steps_done += 1\n",
    "\n",
    "        # ADDITIONS\n",
    "    \n",
    "        for i, len_i in queue_length_tracker.items():\n",
    "            all_avg_queue_lengths[i, episode] = len_i / num_steps\n",
    "            \n",
    "        avg_wait = 0.0\n",
    "        max_wait = 0.0\n",
    "        vehicle_waits = []\n",
    "    \n",
    "        for key in vehicle_wait_tracker:\n",
    "            vehicle_waits.append(vehicle_wait_tracker[key])\n",
    "    \n",
    "        if vehicle_waits:\n",
    "            avg_wait = sum(vehicle_waits) / len(vehicle_waits)\n",
    "            max_wait = max(vehicle_waits)\n",
    "    \n",
    "        avg_wait_per_ep.append(avg_wait)\n",
    "        max_wait_per_ep.append(max_wait)\n",
    "        # \n",
    "    \n",
    "        # Decay epsilon\n",
    "        epsilon = max(min_epsilon, epsilon_decay * epsilon)\n",
    "        print(episode_reward)\n",
    "        rewards_per_episode.append(episode_reward)\n",
    "    \n",
    "    return rewards_per_episode, avg_wait_per_ep, max_wait_per_ep, all_avg_queue_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51bf513d-c28b-461f-9a23-26d9eef8115e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\n",
      "tensor(-12703.)\n",
      "Episode 1\n",
      "tensor(-14264.)\n",
      "Episode 2\n",
      "tensor(-13250.)\n",
      "Episode 3\n",
      "tensor(-530.)\n"
     ]
    }
   ],
   "source": [
    "rewards_per_episode, avg_wait_per_ep, max_wait_per_ep, all_avg_queue_lengths = train_algorithm(4, epsilon, epsilon_decay, min_epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8ad3db-b0af-43e9-8508-2a6c158235c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy_net.state_dict(), \"dqn_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55f59140-1f51-493b-af96-b0a708ea333e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Total reward per episode\n",
    "plt.figure()\n",
    "plt.plot(rewards_per_episode)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.title(\"DQN on traffic lights\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"Plots/rewards_per_episode.png\")\n",
    "plt.close()\n",
    "\n",
    "# Average wait time per episode\n",
    "plt.figure()\n",
    "plt.plot(avg_wait_per_ep)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Average Wait\")\n",
    "plt.title(\"DQN on traffic lights - Avg Wait\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"Plots/avg_wait_per_episode.png\")\n",
    "plt.close()\n",
    "\n",
    "# Maximum wait time per episode\n",
    "plt.figure()\n",
    "plt.plot(max_wait_per_ep)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Maximum Wait\")\n",
    "plt.title(\"DQN on traffic lights - Max Wait\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"Plots/max_wait_per_episode.png\")\n",
    "plt.close()\n",
    "\n",
    "# Per-lane average queue length\n",
    "num_lanes = all_avg_queue_lengths.shape[0]\n",
    "for lane_index in range(num_lanes):\n",
    "    plt.figure()\n",
    "    plt.plot(all_avg_queue_lengths[lane_index].numpy())\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Avg Queue Length\")\n",
    "    plt.title(f\"Avg queue length per episode for lane {lane_index}\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"Plots/avg_queue_lane_{lane_index}.png\")\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "953d9042-be01-451f-86ac-b4160bd473bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_baseline_results(episodes=4):\n",
    "    baseline_rewards_per_episode = []\n",
    "    baseline_avg_wait_per_ep = []\n",
    "    baseline_max_wait_per_ep = []\n",
    "\n",
    "    num_lanes = len(lane_detectors)\n",
    "    baseline_avg_queue_lengths = torch.zeros(num_lanes, episodes)\n",
    "\n",
    "    phase_sequence = [2, 6, 4, 0]  # green phases\n",
    "    green_duration = DELTA_PHASE_DURATION\n",
    "    yellow_duration = YELLOW_PHASE_DURATION\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        print(f\"[Baseline] Episode {episode}\")\n",
    "        change_env()\n",
    "\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "\n",
    "        vehicle_wait_tracker = {}\n",
    "        queue_length_tracker = {}\n",
    "        num_steps = 0\n",
    "\n",
    "        state = get_current_state()\n",
    "        phase_index = 0\n",
    "\n",
    "        while not done:\n",
    "            green_phase = phase_sequence[phase_index]\n",
    "            yellow_phase = green_phase + 1\n",
    "\n",
    "            # Set green phase\n",
    "            traci.trafficlight.setPhase(TRAFFIC_LIGHT_ID, green_phase)\n",
    "            simulate_time(green_duration)\n",
    "            num_steps += green_duration * 20\n",
    "\n",
    "            # Update state and reward\n",
    "            state = get_current_state()\n",
    "            queue_size = torch.sum(state)\n",
    "            reward = -queue_size\n",
    "            episode_reward += reward\n",
    "\n",
    "            for v_id in traci.vehicle.getIDList():\n",
    "                wait_time = traci.vehicle.getWaitingTime(v_id)\n",
    "                if v_id not in vehicle_wait_tracker:\n",
    "                    vehicle_wait_tracker[v_id] = wait_time\n",
    "                elif wait_time > vehicle_wait_tracker[v_id]:\n",
    "                    vehicle_wait_tracker[v_id] = wait_time\n",
    "\n",
    "            curr_queue = get_queue_length()\n",
    "            for i in range(len(curr_queue)):\n",
    "                if i not in queue_length_tracker:\n",
    "                    queue_length_tracker[i] = curr_queue[i]\n",
    "                else:\n",
    "                    queue_length_tracker[i] += curr_queue[i]\n",
    "\n",
    "            done = traci.simulation.getMinExpectedNumber() == 0\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            # Set yellow phase\n",
    "            traci.trafficlight.setPhase(TRAFFIC_LIGHT_ID, yellow_phase)\n",
    "            simulate_time(yellow_duration)\n",
    "            num_steps += yellow_duration * 20\n",
    "\n",
    "            done = traci.simulation.getMinExpectedNumber() == 0\n",
    "            phase_index = (phase_index + 1) % len(phase_sequence)\n",
    "\n",
    "        # Lane-wise average queue lengths\n",
    "        for i, len_i in queue_length_tracker.items():\n",
    "            baseline_avg_queue_lengths[i, episode] = len_i / num_steps\n",
    "\n",
    "        # Wait time stats\n",
    "        vehicle_waits = list(vehicle_wait_tracker.values())\n",
    "        avg_wait = sum(vehicle_waits) / len(vehicle_waits) if vehicle_waits else 0.0\n",
    "        max_wait = max(vehicle_waits) if vehicle_waits else 0.0\n",
    "\n",
    "        baseline_avg_wait_per_ep.append(avg_wait)\n",
    "        baseline_max_wait_per_ep.append(max_wait)\n",
    "        baseline_rewards_per_episode.append(episode_reward)\n",
    "\n",
    "    return (\n",
    "        baseline_rewards_per_episode,\n",
    "        baseline_avg_wait_per_ep,\n",
    "        baseline_max_wait_per_ep,\n",
    "        baseline_avg_queue_lengths\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96d2b7f6-3917-40a7-a658-12700977d5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Baseline] Episode 0\n",
      "[Baseline] Episode 1\n",
      "[Baseline] Episode 2\n",
      "[Baseline] Episode 3\n"
     ]
    }
   ],
   "source": [
    "baseline_results = get_baseline_results(episodes=4)\n",
    "baseline_rewards, baseline_avg_waits, baseline_max_waits, baseline_queues = baseline_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fd057d1-a4cc-40a5-b4dd-2460de4f3de8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(-6354.), tensor(-14060.), tensor(-12336.), tensor(-521.)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcaf1cd0-41b2-4510-8c52-863ca764d014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14.694024249422625, 19.395777906304208, 17.880480868665963, 12.91672661870504]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_avg_waits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21e7db3b-cbbe-4bbe-a9f3-509e38ba0607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[34.05, 90.65, 64.05, 28.4]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_max_waits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "021cd574-dd4a-4188-8f6c-b44c81b4bcbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0046, 0.0026, 0.0033, 0.0000],\n",
       "        [0.0021, 0.0020, 0.0023, 0.0000],\n",
       "        [0.0202, 0.0230, 0.0030, 0.0000],\n",
       "        [0.0237, 0.0424, 0.0024, 0.0000],\n",
       "        [0.0037, 0.0028, 0.0347, 0.0000],\n",
       "        [0.0017, 0.0014, 0.0187, 0.0000],\n",
       "        [0.0136, 0.0271, 0.0319, 0.0044],\n",
       "        [0.0162, 0.0482, 0.0443, 0.0027]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_queues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a9c553-9bc0-4e51-b73d-27a47ebdf4c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
