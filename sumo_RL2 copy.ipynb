{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dqn_hyperparam_tuning.ipynb\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import traci\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SUMO Configuration ---\n",
    "def sumo_config(traffic_pattern=\"P1\"):\n",
    "    sumo_config = [\n",
    "        \"sumo\",\n",
    "        \"-c\", f\"SUMO_networks/{traffic_pattern}/junction.sumocfg\",\n",
    "        \"--step-length\", \"0.05\",\n",
    "        \"--delay\", \"0\",\n",
    "        \"--lateral-resolution\", \"0.1\",\n",
    "        \"--start\",\n",
    "        \"--no-warnings\",\n",
    "        \"--no-step-log\",\n",
    "    ]\n",
    "    return sumo_config\n",
    "\n",
    "if \"SUMO_HOME\" in os.environ:\n",
    "    tools = os.path.join(os.environ[\"SUMO_HOME\"], \"tools\")\n",
    "    sys.path.append(tools)\n",
    "else:\n",
    "    sys.exit(\"Please declare environment variable 'SUMO_HOME'\")\n",
    "\n",
    "if not traci.isLoaded():\n",
    "    traci.start(sumo_config())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Global Variables ---\n",
    "action_space_size = 8\n",
    "TRAFFIC_LIGHT_ID = \"traffic_light\"\n",
    "DELTA_PHASE_DURATION = 6\n",
    "YELLOW_PHASE_DURATION = 4\n",
    "lane_detectors = [f'q{i+1}' for i in range(8)]\n",
    "current_phase = 2\n",
    "\n",
    "def get_queue_length():\n",
    "    return torch.tensor([\n",
    "        traci.lanearea.getLastStepHaltingNumber(d) for d in lane_detectors\n",
    "    ], dtype=torch.float)\n",
    "\n",
    "def get_current_state():\n",
    "    return get_queue_length()\n",
    "\n",
    "def simulate_time(seconds=1):\n",
    "    for _ in range(20 * seconds):\n",
    "        traci.simulationStep()\n",
    "\n",
    "def step(action):\n",
    "    global current_phase\n",
    "    if 2 * action == current_phase:\n",
    "        traci.trafficlight.setPhase(TRAFFIC_LIGHT_ID, 2 * action)\n",
    "        simulate_time(DELTA_PHASE_DURATION)\n",
    "    else:\n",
    "        traci.trafficlight.setPhase(TRAFFIC_LIGHT_ID, current_phase + 1)\n",
    "        simulate_time(YELLOW_PHASE_DURATION)\n",
    "        current_phase = 2 * action\n",
    "        traci.trafficlight.setPhase(TRAFFIC_LIGHT_ID, 2 * action)\n",
    "        simulate_time(DELTA_PHASE_DURATION)\n",
    "    next_state = get_current_state()\n",
    "    reward = -torch.sum(next_state)\n",
    "    done = traci.simulation.getMinExpectedNumber() == 0\n",
    "    return next_state, reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Neural Network ---\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 64), nn.ReLU(),\n",
    "            nn.Linear(64, action_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Environment Switching ---\n",
    "import itertools\n",
    "traffic_patterns = itertools.cycle([\"P1\", \"P2\", \"P3\", \"P4\"])\n",
    "\n",
    "def change_env():\n",
    "    pattern = next(traffic_patterns)\n",
    "    if traci.isLoaded():\n",
    "        traci.close()\n",
    "    traci.start(sumo_config(pattern))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Choose Action ---\n",
    "def choose_action(state, epsilon, policy_net):\n",
    "    if random.random() < epsilon:\n",
    "        return random.randint(0, action_space_size - 1)\n",
    "    else:\n",
    "        return torch.argmax(policy_net(state.unsqueeze(0))).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Optimizer Step ---\n",
    "def optimise_model(policy_net, target_net, memory, optimizer, batch_size, gamma):\n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "    batch = random.sample(memory, batch_size)\n",
    "    states = torch.stack([x[0] for x in batch])\n",
    "    actions = torch.tensor([x[1] for x in batch]).unsqueeze(1)\n",
    "    rewards = torch.tensor([x[2] for x in batch], dtype=torch.float)\n",
    "    next_states = torch.stack([x[3] for x in batch])\n",
    "    dones = torch.tensor([x[4] for x in batch], dtype=torch.float)\n",
    "\n",
    "    q_vals = policy_net(states).gather(1, actions).squeeze()\n",
    "    with torch.no_grad():\n",
    "        max_next_q_vals = target_net(next_states).max(1)[0]\n",
    "        target_vals = rewards + gamma * max_next_q_vals * (1 - dones)\n",
    "    loss = nn.MSELoss()(q_vals, target_vals)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training Loop ---\n",
    "def train_algorithm(params, episodes=10):\n",
    "    gamma = params[\"gamma\"]\n",
    "    epsilon = params[\"epsilon\"]\n",
    "    epsilon_decay = params[\"epsilon_decay\"]\n",
    "    min_epsilon = params[\"min_epsilon\"]\n",
    "    lr = params[\"learning_rate\"]\n",
    "    batch_size = params[\"batch_size\"]\n",
    "    target_update_freq = params[\"target_update_freq\"]\n",
    "    memory_size = params[\"memory_size\"]\n",
    "\n",
    "    state_dim = len(lane_detectors)\n",
    "    policy_net = DQN(state_dim, action_space_size)\n",
    "    target_net = DQN(state_dim, action_space_size)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=lr)\n",
    "    memory = deque(maxlen=memory_size)\n",
    "\n",
    "    rewards_per_episode = []\n",
    "    steps_done = 0\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        change_env()\n",
    "        state = get_current_state()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = choose_action(state, epsilon, policy_net)\n",
    "            next_state, reward, done = step(action)\n",
    "            memory.append((state, action, reward, next_state, done))\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            optimise_model(policy_net, target_net, memory, optimizer, batch_size, gamma)\n",
    "            if steps_done % target_update_freq == 0:\n",
    "                target_net.load_state_dict(policy_net.state_dict())\n",
    "            steps_done += 1\n",
    "\n",
    "        epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "        rewards_per_episode.append(episode_reward.item())\n",
    "\n",
    "    return sum(rewards_per_episode) / episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544ec7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Grid Search ---\n",
    "param_grid = {\n",
    "    \"gamma\": [0.99],\n",
    "    \"epsilon\": [0.9],\n",
    "    \"epsilon_decay\": [0.95],\n",
    "    \"min_epsilon\": [0.05],\n",
    "    \"learning_rate\": [0.001],\n",
    "    \"batch_size\": [128],\n",
    "    \"target_update_freq\": [400],\n",
    "    \"memory_size\": [10000],\n",
    "}\n",
    "\n",
    "keys, values = zip(*param_grid.items())\n",
    "param_combinations = [dict(zip(keys, v)) for v in product(*values)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running combo: {'gamma': 0.99, 'epsilon': 1.0, 'epsilon_decay': 0.99, 'min_epsilon': 0.05, 'learning_rate': 0.001, 'batch_size': 64, 'target_update_freq': 100, 'memory_size': 5000}\n",
      "Running combo: {'gamma': 0.7, 'epsilon': 1.0, 'epsilon_decay': 0.99, 'min_epsilon': 0.05, 'learning_rate': 0.001, 'batch_size': 64, 'target_update_freq': 100, 'memory_size': 5000}\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for combo in param_combinations:\n",
    "    print(f\"Running combo: {combo}\")\n",
    "    avg_reward = train_algorithm(combo, episodes=150)\n",
    "    combo_result = combo.copy()\n",
    "    combo_result[\"avg_reward\"] = avg_reward\n",
    "    results.append(combo_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(\"grid_search_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   gamma  epsilon  epsilon_decay  min_epsilon  learning_rate  batch_size  \\\n",
      "0   0.99      1.0           0.99         0.05          0.001          64   \n",
      "1   0.70      1.0           0.99         0.05          0.001          64   \n",
      "\n",
      "   target_update_freq  memory_size  avg_reward  \n",
      "0                 100         5000    -11777.0  \n",
      "1                 100         5000    -17974.0  \n"
     ]
    }
   ],
   "source": [
    "# Show top 5 configs\n",
    "print(results_df.sort_values(\"avg_reward\", ascending=False).head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "a1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
