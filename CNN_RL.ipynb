{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dqn_hyperparam_tuning.ipynb\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import traci\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SUMO Configuration ---\n",
    "def sumo_config(traffic_pattern=\"P1\"):\n",
    "    sumo_config = [\n",
    "        \"sumo\",\n",
    "        \"-c\", f\"SUMO_networks/{traffic_pattern}/junction.sumocfg\",\n",
    "        \"--step-length\", \"0.05\",\n",
    "        \"--delay\", \"0\",\n",
    "        \"--lateral-resolution\", \"0.1\",\n",
    "        \"--start\",\n",
    "        \"--no-warnings\",\n",
    "        \"--no-step-log\",\n",
    "    ]\n",
    "    return sumo_config\n",
    "\n",
    "if \"SUMO_HOME\" in os.environ:\n",
    "    tools = os.path.join(os.environ[\"SUMO_HOME\"], \"tools\")\n",
    "    sys.path.append(tools)\n",
    "else:\n",
    "    sys.exit(\"Please declare environment variable 'SUMO_HOME'\")\n",
    "\n",
    "if not traci.isLoaded():\n",
    "    traci.start(sumo_config())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Global Variables ---\n",
    "action_space_size = 8\n",
    "TRAFFIC_LIGHT_ID = \"traffic_light\"\n",
    "DELTA_PHASE_DURATION = 6\n",
    "YELLOW_PHASE_DURATION = 4\n",
    "lane_detectors = [f'q{i+1}' for i in range(8)]\n",
    "current_phase = 2\n",
    "frame_buffer = deque(maxlen=4)\n",
    "\n",
    "def get_queue_length():\n",
    "    return torch.tensor([\n",
    "        traci.lanearea.getLastStepHaltingNumber(d) for d in lane_detectors\n",
    "    ], dtype=torch.float)\n",
    "\n",
    "def generate_occupancy_grid(grid_size=(84, 84),\n",
    "                             bounds=(-91.5, 76.5, -66.5, 101.5)):\n",
    "    x_min, x_max, y_min, y_max = bounds\n",
    "    x_scale = grid_size[1] / (x_max - x_min)\n",
    "    y_scale = grid_size[0] / (y_max - y_min)\n",
    "\n",
    "    grid = torch.zeros(grid_size, dtype=torch.float32)\n",
    "\n",
    "    for v_id in traci.vehicle.getIDList():\n",
    "        x, y = traci.vehicle.getPosition(v_id)\n",
    "\n",
    "        if x_min <= x <= x_max and y_min <= y <= y_max:\n",
    "            col = int((x - x_min) * x_scale)\n",
    "            row = int((y - y_min) * y_scale)\n",
    "\n",
    "            if 0 <= row < grid_size[0] and 0 <= col < grid_size[1]:\n",
    "                grid[row, col] = 1.0  \n",
    "\n",
    "    return grid\n",
    "\n",
    "def get_current_state():\n",
    "    frame_buffer.clear()  # clear previous frames to maintain consistency\n",
    "\n",
    "    for _ in range(4):\n",
    "        simulate_time(1)  # simulate 1 second\n",
    "        grid = generate_occupancy_grid()\n",
    "        frame_buffer.append(grid.unsqueeze(0))\n",
    "\n",
    "    # if len(frame_buffer) >= 2:\n",
    "    #     second_recent = frame_buffer[-2].squeeze(0)  # shape: [84, 84]\n",
    "    #     plt.figure(figsize=(6, 6))\n",
    "    #     plt.imshow(second_recent.numpy(), cmap='gray', origin='lower')\n",
    "    #     plt.title(\"Previous Occupancy Grid\")\n",
    "    #     plt.xlabel(\"X-axis (cols)\")\n",
    "    #     plt.ylabel(\"Y-axis (rows)\")\n",
    "    #     plt.colorbar(label='Occupancy')\n",
    "    #     plt.grid(False)\n",
    "    #     plt.tight_layout()\n",
    "    #     plt.show()\n",
    "\n",
    "    # # Plot latest grid\n",
    "    # plt.figure(figsize=(6, 6))\n",
    "    # plt.imshow(grid.numpy(), cmap='gray', origin='lower')\n",
    "    # plt.title(\"Latest Occupancy Grid\")\n",
    "    # plt.xlabel(\"X-axis (cols)\")\n",
    "    # plt.ylabel(\"Y-axis (rows)\")\n",
    "    # plt.colorbar(label='Occupancy')\n",
    "    # plt.grid(False)\n",
    "    # plt.tight_layout()\n",
    "    # plt.show()\n",
    "    \n",
    "    state = torch.cat(list(frame_buffer), dim=0)\n",
    "    return state\n",
    "    \n",
    "\n",
    "def simulate_time(seconds=1):\n",
    "    for _ in range(20 * seconds):\n",
    "        traci.simulationStep()\n",
    "\n",
    "def step(action):\n",
    "    global current_phase\n",
    "    if 2 * action == current_phase:\n",
    "        traci.trafficlight.setPhase(TRAFFIC_LIGHT_ID, 2 * action)\n",
    "        simulate_time(DELTA_PHASE_DURATION - 4)\n",
    "    else:\n",
    "        traci.trafficlight.setPhase(TRAFFIC_LIGHT_ID, current_phase + 1)\n",
    "        simulate_time(YELLOW_PHASE_DURATION)\n",
    "        current_phase = 2 * action\n",
    "        traci.trafficlight.setPhase(TRAFFIC_LIGHT_ID, 2 * action)\n",
    "        simulate_time(DELTA_PHASE_DURATION - 4)\n",
    "    next_state = get_current_state()\n",
    "    queue_length = get_queue_length()\n",
    "    reward = -torch.sum(queue_length)\n",
    "    done = traci.simulation.getMinExpectedNumber() == 0\n",
    "    return next_state, reward, done, queue_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5fa66a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def save_episode_plots(metrics, combo_name=\"default\"):\n",
    "    os.makedirs(f\"Plots/{combo_name}\", exist_ok=True)\n",
    "\n",
    "    # Plot total reward per episode\n",
    "    plt.figure()\n",
    "    plt.plot(metrics[\"rewards_per_episode\"])\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Total Reward\")\n",
    "    plt.title(f\"{combo_name} - Reward per Episode\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"Plots/{combo_name}/rewards_per_episode.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Plot average wait per episode\n",
    "    plt.figure()\n",
    "    plt.plot(metrics[\"avg_wait_per_ep\"])\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Average Wait Time\")\n",
    "    plt.title(f\"{combo_name} - Avg Wait per Episode\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"Plots/{combo_name}/avg_wait_per_episode.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Plot maximum wait per episode\n",
    "    plt.figure()\n",
    "    plt.plot(metrics[\"max_wait_per_ep\"])\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Maximum Wait Time\")\n",
    "    plt.title(f\"{combo_name} - Max Wait per Episode\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"Plots/{combo_name}/max_wait_per_episode.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Per-lane average queue length plots\n",
    "    avg_queues = metrics[\"avg_queue_lengths\"]\n",
    "    for lane_index in range(avg_queues.shape[0]):\n",
    "        plt.figure()\n",
    "        plt.plot(avg_queues[lane_index].numpy())\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Average Queue Length\")\n",
    "        plt.title(f\"{combo_name} - Lane {lane_index} Avg Queue Length\")\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"Plots/{combo_name}/avg_queue_lane_{lane_index}.png\")\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a81015ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, num_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=4, out_channels=32, kernel_size=8, stride=4)  \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)  \n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)  \n",
    "        \n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 512)\n",
    "        self.output = nn.Linear(512, num_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x / 255.0  \n",
    "        x = F.relu(self.conv1(x))   \n",
    "        x = F.relu(self.conv2(x))  \n",
    "        x = F.relu(self.conv3(x))   \n",
    "        \n",
    "        x = x.view(x.size(0), -1)   \n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.output(x)      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ecc0077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Environment Switching ---\n",
    "import itertools\n",
    "traffic_patterns = itertools.cycle([\"P1\", \"P2\", \"P3\", \"P4\"])\n",
    "\n",
    "def change_env():\n",
    "    pattern = next(traffic_patterns)\n",
    "    if traci.isLoaded():\n",
    "        traci.close()\n",
    "    traci.start(sumo_config(pattern))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "535d423d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Choose Action ---\n",
    "def choose_action(state, epsilon, policy_net):\n",
    "    if random.random() < epsilon:\n",
    "        return random.randint(0, action_space_size - 1)\n",
    "    else:\n",
    "        return torch.argmax(policy_net(state.unsqueeze(0))).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "120e33f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Optimizer Step ---\n",
    "def optimise_model(policy_net, target_net, memory, optimizer, batch_size, gamma):\n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "    batch = random.sample(memory, batch_size)\n",
    "    states = torch.stack([x[0] for x in batch])\n",
    "    actions = torch.tensor([x[1] for x in batch]).unsqueeze(1)\n",
    "    rewards = torch.tensor([x[2] for x in batch], dtype=torch.float)\n",
    "    next_states = torch.stack([x[3] for x in batch])\n",
    "    dones = torch.tensor([x[4] for x in batch], dtype=torch.float)\n",
    "\n",
    "    q_vals = policy_net(states).gather(1, actions).squeeze()\n",
    "    with torch.no_grad():\n",
    "        max_next_q_vals = target_net(next_states).max(1)[0]\n",
    "        target_vals = rewards + gamma * max_next_q_vals * (1 - dones)\n",
    "    loss = nn.MSELoss()(q_vals, target_vals)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccb4bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training Loop ---\n",
    "def train_algorithm(params, episodes=1):\n",
    "    gamma = params[\"gamma\"]\n",
    "    epsilon = params[\"epsilon\"]\n",
    "    epsilon_decay = params[\"epsilon_decay\"]\n",
    "    min_epsilon = params[\"min_epsilon\"]\n",
    "    lr = params[\"learning_rate\"]\n",
    "    batch_size = params[\"batch_size\"]\n",
    "    target_update_freq = params[\"target_update_freq\"]\n",
    "    memory_size = params[\"memory_size\"]\n",
    "\n",
    "    policy_net = DQN(action_space_size)\n",
    "    target_net = DQN(action_space_size)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=lr)\n",
    "    memory = deque(maxlen=memory_size)\n",
    "\n",
    "    rewards_per_episode = []\n",
    "    steps_done = 0\n",
    "\n",
    "    avg_wait_per_ep = []\n",
    "    max_wait_per_ep = []\n",
    "    all_avg_queue_lengths = torch.zeros(len(lane_detectors), episodes)\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        change_env()\n",
    "        state = get_current_state()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "\n",
    "        vehicle_wait_tracker = {}\n",
    "        queue_length_tracker = {}\n",
    "        num_steps = 0\n",
    "\n",
    "        while not done:\n",
    "            action = choose_action(state, epsilon, policy_net)\n",
    "            next_state, reward, done, curr_queue = step(action)\n",
    "            memory.append((state, action, reward, next_state, done))\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "\n",
    "            # Vehicle wait time tracking\n",
    "            for v_id in traci.vehicle.getIDList():\n",
    "                wait_time = traci.vehicle.getWaitingTime(v_id)\n",
    "                if v_id not in vehicle_wait_tracker:\n",
    "                    vehicle_wait_tracker[v_id] = wait_time\n",
    "                elif wait_time > vehicle_wait_tracker[v_id]:\n",
    "                    vehicle_wait_tracker[v_id] = wait_time\n",
    "\n",
    "            # Queue length tracking\n",
    "            for i in range(len(curr_queue)):\n",
    "                queue_length_tracker[i] = queue_length_tracker.get(i, 0) + curr_queue[i]\n",
    "\n",
    "            optimise_model(policy_net, target_net, memory, optimizer, batch_size, gamma)\n",
    "            if steps_done % target_update_freq == 0:\n",
    "                target_net.load_state_dict(policy_net.state_dict())\n",
    "            steps_done += 1\n",
    "            num_steps += 1\n",
    "\n",
    "        # Aggregate queue stats\n",
    "        for i, total_len in queue_length_tracker.items():\n",
    "            all_avg_queue_lengths[i, episode] = total_len / num_steps\n",
    "\n",
    "        vehicle_waits = list(vehicle_wait_tracker.values())\n",
    "        avg_wait = sum(vehicle_waits) / len(vehicle_waits) if vehicle_waits else 0.0\n",
    "        max_wait = max(vehicle_waits) if vehicle_waits else 0.0\n",
    "        avg_wait_per_ep.append(avg_wait)\n",
    "        max_wait_per_ep.append(max_wait)\n",
    "\n",
    "        epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "        rewards_per_episode.append(episode_reward.item())\n",
    "        print(episode_reward)\n",
    "\n",
    "    N = max(1, int(0.2 * episodes))  # 20% of episodes, at least 1\n",
    "    avg_reward_last_N = sum(rewards_per_episode[-N:]) / N\n",
    "    avg_wait_last_N = sum(avg_wait_per_ep[-N:]) / N\n",
    "    max_wait_last_N = max(max_wait_per_ep[-N:]) if max_wait_per_ep[-N:] else 0.0\n",
    "\n",
    "    return {\n",
    "        \"avg_reward\": sum(rewards_per_episode) / episodes,\n",
    "        \"avg_wait_per_ep\": avg_wait_per_ep,\n",
    "        \"max_wait_per_ep\": max_wait_per_ep,\n",
    "        \"avg_queue_lengths\": all_avg_queue_lengths,\n",
    "        \"rewards_per_episode\": rewards_per_episode,\n",
    "        \"avg_reward_last_N\": avg_reward_last_N,\n",
    "        \"avg_wait_last_N\": avg_wait_last_N,\n",
    "        \"max_wait_last_N\": max_wait_last_N,\n",
    "        \"trained_model\": policy_net\n",
    "    }\n",
    "\n",
    "    # recent_rewards = rewards_per_episode[-40:] if len(rewards_per_episode) >= 40 else rewards_per_episode\n",
    "    # return sum(recent_rewards) / len(recent_rewards), rewards_per_episode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "544ec7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Grid Search ---\n",
    "param_grid = {\n",
    "    \"gamma\": [0.99],\n",
    "    \"epsilon\": [0.9],\n",
    "    \"epsilon_decay\": [0.95],\n",
    "    \"min_epsilon\": [0.05],\n",
    "    \"learning_rate\": [0.001],\n",
    "    \"batch_size\": [128],\n",
    "    \"target_update_freq\": [1800],\n",
    "    \"memory_size\": [40000],\n",
    "}\n",
    "\n",
    "keys, values = zip(*param_grid.items())\n",
    "param_combinations = [dict(zip(keys, v)) for v in product(*values)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf996bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running combo: {'gamma': 0.99, 'epsilon': 0.9, 'epsilon_decay': 0.95, 'min_epsilon': 0.05, 'learning_rate': 0.001, 'batch_size': 128, 'target_update_freq': 1800, 'memory_size': 40000}\n",
      "Episode: 0\n",
      "tensor(-6986.)\n",
      "Episode: 1\n",
      "tensor(-8879.)\n",
      "Episode: 2\n",
      "tensor(-6225.)\n",
      "Episode: 3\n",
      "tensor(-437.)\n",
      "Episode: 4\n",
      "tensor(-3567.)\n",
      "Episode: 5\n",
      "tensor(-3542.)\n",
      "Episode: 6\n",
      "tensor(-5549.)\n",
      "Episode: 7\n",
      "tensor(-147.)\n",
      "Episode: 8\n",
      "tensor(-2564.)\n",
      "Episode: 9\n",
      "tensor(-4203.)\n",
      "Episode: 10\n",
      "tensor(-2384.)\n",
      "Episode: 11\n",
      "tensor(-181.)\n",
      "Episode: 12\n",
      "tensor(-2197.)\n",
      "Episode: 13\n",
      "tensor(-2065.)\n",
      "Episode: 14\n",
      "tensor(-2343.)\n",
      "Episode: 15\n",
      "tensor(-87.)\n",
      "Episode: 16\n",
      "tensor(-1636.)\n",
      "Episode: 17\n",
      "tensor(-2087.)\n",
      "Episode: 18\n",
      "tensor(-3012.)\n",
      "Episode: 19\n",
      "tensor(-59.)\n",
      "Episode: 20\n",
      "tensor(-1607.)\n",
      "Episode: 21\n",
      "tensor(-2207.)\n",
      "Episode: 22\n",
      "tensor(-2203.)\n",
      "Episode: 23\n",
      "tensor(-30.)\n",
      "Episode: 24\n",
      "tensor(-1572.)\n",
      "Episode: 25\n",
      "tensor(-1723.)\n",
      "Episode: 26\n",
      "tensor(-2486.)\n",
      "Episode: 27\n",
      "tensor(-36.)\n",
      "Episode: 28\n",
      "tensor(-1378.)\n",
      "Episode: 29\n",
      "tensor(-1571.)\n",
      "Episode: 30\n",
      "tensor(-2062.)\n",
      "Episode: 31\n",
      "tensor(-28.)\n",
      "Episode: 32\n",
      "tensor(-1358.)\n",
      "Episode: 33\n",
      "tensor(-1689.)\n",
      "Episode: 34\n",
      "tensor(-2269.)\n",
      "Episode: 35\n",
      "tensor(-33.)\n",
      "Episode: 36\n",
      "tensor(-1522.)\n",
      "Episode: 37\n",
      "tensor(-1632.)\n",
      "Episode: 38\n",
      "tensor(-1974.)\n",
      "Episode: 39\n",
      "tensor(-27.)\n",
      "Episode: 40\n",
      "tensor(-1317.)\n",
      "Episode: 41\n",
      "tensor(-2095.)\n",
      "Episode: 42\n",
      "tensor(-1886.)\n",
      "Episode: 43\n",
      "tensor(-26.)\n",
      "Episode: 44\n",
      "tensor(-1300.)\n",
      "Episode: 45\n",
      "tensor(-1375.)\n",
      "Episode: 46\n",
      "tensor(-1807.)\n",
      "Episode: 47\n",
      "tensor(-28.)\n",
      "Episode: 48\n",
      "tensor(-1163.)\n",
      "Episode: 49\n",
      "tensor(-1410.)\n",
      "Episode: 50\n",
      "tensor(-1569.)\n",
      "Episode: 51\n",
      "tensor(-20.)\n",
      "Episode: 52\n",
      "tensor(-1283.)\n",
      "Episode: 53\n",
      "tensor(-1296.)\n",
      "Episode: 54\n",
      "tensor(-1846.)\n",
      "Episode: 55\n",
      "tensor(-17.)\n",
      "Episode: 56\n",
      "tensor(-1192.)\n",
      "Episode: 57\n",
      "tensor(-1349.)\n",
      "Episode: 58\n",
      "tensor(-1537.)\n",
      "Episode: 59\n",
      "tensor(-23.)\n",
      "Episode: 60\n",
      "tensor(-1218.)\n",
      "Episode: 61\n",
      "tensor(-1263.)\n",
      "Episode: 62\n",
      "tensor(-1376.)\n",
      "Episode: 63\n",
      "tensor(-20.)\n",
      "Episode: 64\n",
      "tensor(-1171.)\n",
      "Episode: 65\n",
      "tensor(-1333.)\n",
      "Episode: 66\n",
      "tensor(-1550.)\n",
      "Episode: 67\n",
      "tensor(-24.)\n",
      "Episode: 68\n",
      "tensor(-1248.)\n",
      "Episode: 69\n",
      "tensor(-1274.)\n",
      "Episode: 70\n",
      "tensor(-1622.)\n",
      "Episode: 71\n",
      "tensor(-26.)\n",
      "Episode: 72\n",
      "tensor(-1233.)\n",
      "Episode: 73\n",
      "tensor(-1301.)\n",
      "Episode: 74\n",
      "tensor(-1460.)\n",
      "Episode: 75\n",
      "tensor(-40.)\n",
      "Episode: 76\n",
      "tensor(-1205.)\n",
      "Episode: 77\n",
      "tensor(-1315.)\n",
      "Episode: 78\n",
      "tensor(-1635.)\n",
      "Episode: 79\n",
      "tensor(-36.)\n",
      "Episode: 80\n",
      "tensor(-1248.)\n",
      "Episode: 81\n",
      "tensor(-1291.)\n",
      "Episode: 82\n",
      "tensor(-1566.)\n",
      "Episode: 83\n",
      "tensor(-34.)\n",
      "Episode: 84\n",
      "tensor(-1223.)\n",
      "Episode: 85\n",
      "tensor(-1405.)\n",
      "Episode: 86\n",
      "tensor(-1546.)\n",
      "Episode: 87\n",
      "tensor(-37.)\n",
      "Episode: 88\n",
      "tensor(-1214.)\n",
      "Episode: 89\n",
      "tensor(-1345.)\n",
      "Episode: 90\n",
      "tensor(-1912.)\n",
      "Episode: 91\n",
      "tensor(-53.)\n",
      "Episode: 92\n",
      "tensor(-1225.)\n",
      "Episode: 93\n",
      "tensor(-1851.)\n",
      "Episode: 94\n",
      "tensor(-1506.)\n",
      "Episode: 95\n",
      "tensor(-27.)\n",
      "Episode: 96\n",
      "tensor(-1216.)\n",
      "Episode: 97\n",
      "tensor(-1379.)\n",
      "Episode: 98\n",
      "tensor(-1638.)\n",
      "Episode: 99\n",
      "tensor(-39.)\n",
      "Episode: 100\n",
      "tensor(-1193.)\n",
      "Episode: 101\n",
      "tensor(-1521.)\n",
      "Episode: 102\n",
      "tensor(-1561.)\n",
      "Episode: 103\n",
      "tensor(-43.)\n",
      "Episode: 104\n",
      "tensor(-1120.)\n",
      "Episode: 105\n",
      "tensor(-1481.)\n",
      "Episode: 106\n",
      "tensor(-1426.)\n",
      "Episode: 107\n",
      "tensor(-30.)\n",
      "Episode: 108\n",
      "tensor(-1337.)\n",
      "Episode: 109\n",
      "tensor(-1310.)\n",
      "Episode: 110\n",
      "tensor(-1696.)\n",
      "Episode: 111\n",
      "tensor(-39.)\n",
      "Episode: 112\n",
      "tensor(-1226.)\n",
      "Episode: 113\n",
      "tensor(-1505.)\n",
      "Episode: 114\n",
      "tensor(-1630.)\n",
      "Episode: 115\n",
      "tensor(-26.)\n",
      "Episode: 116\n",
      "tensor(-1245.)\n",
      "Episode: 117\n",
      "tensor(-1232.)\n",
      "Episode: 118\n",
      "tensor(-1512.)\n",
      "Episode: 119\n",
      "tensor(-17.)\n",
      "Episode: 120\n",
      "tensor(-1163.)\n",
      "Episode: 121\n",
      "tensor(-1433.)\n",
      "Episode: 122\n",
      "tensor(-1517.)\n",
      "Episode: 123\n",
      "tensor(-30.)\n",
      "Episode: 124\n",
      "tensor(-1199.)\n",
      "Episode: 125\n",
      "tensor(-1414.)\n",
      "Episode: 126\n",
      "tensor(-1544.)\n",
      "Episode: 127\n",
      "tensor(-45.)\n",
      "Episode: 128\n",
      "tensor(-1157.)\n",
      "Episode: 129\n",
      "tensor(-1440.)\n",
      "Episode: 130\n",
      "tensor(-1651.)\n",
      "Episode: 131\n",
      "tensor(-39.)\n",
      "Episode: 132\n",
      "tensor(-1177.)\n",
      "Episode: 133\n",
      "tensor(-1264.)\n",
      "Episode: 134\n",
      "tensor(-1599.)\n",
      "Episode: 135\n",
      "tensor(-34.)\n",
      "Episode: 136\n",
      "tensor(-1196.)\n",
      "Episode: 137\n",
      "tensor(-1438.)\n",
      "Episode: 138\n",
      "tensor(-1810.)\n",
      "Episode: 139\n",
      "tensor(-31.)\n",
      "Episode: 140\n",
      "tensor(-1209.)\n",
      "Episode: 141\n",
      "tensor(-1371.)\n",
      "Episode: 142\n",
      "tensor(-1590.)\n",
      "Episode: 143\n",
      "tensor(-30.)\n",
      "Episode: 144\n",
      "tensor(-1104.)\n",
      "Episode: 145\n",
      "tensor(-1442.)\n",
      "Episode: 146\n",
      "tensor(-1594.)\n",
      "Episode: 147\n",
      "tensor(-35.)\n",
      "Episode: 148\n",
      "tensor(-1129.)\n",
      "Episode: 149\n",
      "tensor(-1345.)\n",
      "Episode: 150\n",
      "tensor(-1820.)\n",
      "Episode: 151\n",
      "tensor(-42.)\n",
      "Episode: 152\n",
      "tensor(-1127.)\n",
      "Episode: 153\n",
      "tensor(-1315.)\n",
      "Episode: 154\n",
      "tensor(-1651.)\n",
      "Episode: 155\n",
      "tensor(-31.)\n",
      "Episode: 156\n",
      "tensor(-1214.)\n",
      "Episode: 157\n",
      "tensor(-1385.)\n",
      "Episode: 158\n",
      "tensor(-1470.)\n",
      "Episode: 159\n",
      "tensor(-51.)\n",
      "Episode: 160\n",
      "tensor(-1306.)\n",
      "Episode: 161\n",
      "tensor(-1493.)\n",
      "Episode: 162\n",
      "tensor(-1790.)\n",
      "Episode: 163\n",
      "tensor(-45.)\n",
      "Episode: 164\n",
      "tensor(-1259.)\n",
      "Episode: 165\n",
      "tensor(-1508.)\n",
      "Episode: 166\n",
      "tensor(-1572.)\n",
      "Episode: 167\n",
      "tensor(-68.)\n",
      "Episode: 168\n",
      "tensor(-1123.)\n",
      "Episode: 169\n",
      "tensor(-1258.)\n",
      "Episode: 170\n",
      "tensor(-1667.)\n",
      "Episode: 171\n",
      "tensor(-96.)\n",
      "Episode: 172\n",
      "tensor(-1359.)\n",
      "Episode: 173\n",
      "tensor(-1413.)\n",
      "Episode: 174\n",
      "tensor(-1637.)\n",
      "Episode: 175\n",
      "tensor(-59.)\n",
      "Episode: 176\n",
      "tensor(-1259.)\n",
      "Episode: 177\n",
      "tensor(-1361.)\n",
      "Episode: 178\n",
      "tensor(-1527.)\n",
      "Episode: 179\n",
      "tensor(-55.)\n",
      "Episode: 180\n",
      "tensor(-1351.)\n",
      "Episode: 181\n",
      "tensor(-1402.)\n",
      "Episode: 182\n",
      "tensor(-1744.)\n",
      "Episode: 183\n",
      "tensor(-40.)\n",
      "Episode: 184\n",
      "tensor(-1193.)\n",
      "Episode: 185\n",
      "tensor(-2482.)\n",
      "Episode: 186\n",
      "tensor(-2025.)\n",
      "Episode: 187\n",
      "tensor(-113.)\n",
      "Episode: 188\n",
      "tensor(-1202.)\n",
      "Episode: 189\n",
      "tensor(-1296.)\n",
      "Episode: 190\n",
      "tensor(-1714.)\n",
      "Episode: 191\n",
      "tensor(-188.)\n",
      "Episode: 192\n",
      "tensor(-1290.)\n",
      "Episode: 193\n",
      "tensor(-1464.)\n",
      "Episode: 194\n",
      "tensor(-1686.)\n",
      "Episode: 195\n",
      "tensor(-92.)\n",
      "Episode: 196\n",
      "tensor(-1294.)\n",
      "Episode: 197\n",
      "tensor(-2257.)\n",
      "Episode: 198\n",
      "tensor(-1844.)\n",
      "Episode: 199\n",
      "tensor(-41.)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'trained_cnn_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrained_Models\u001b[39m\u001b[38;5;124m\"\u001b[39m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     18\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrained_Models/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcombo_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 19\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(\u001b[43mmetrics\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrained_cnn_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mstate_dict(), model_path)\n\u001b[0;32m     21\u001b[0m num_scenarios \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[0;32m     22\u001b[0m scenario_rewards \u001b[38;5;241m=\u001b[39m [[] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_scenarios)]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'trained_cnn_model'"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for i, combo in enumerate(param_combinations):\n",
    "    if i == 0:\n",
    "        continue\n",
    "    \n",
    "    print(f\"Running combo: {combo}\")\n",
    "    metrics = train_algorithm(combo, episodes=200)\n",
    "    combo_result = combo.copy()\n",
    "    combo_result[\"avg_reward\"] = metrics[\"avg_reward\"]\n",
    "    combo_result[\"avg_reward_last_N\"] = metrics[\"avg_reward_last_N\"]\n",
    "    combo_result[\"avg_wait_last_N\"] = metrics[\"avg_wait_last_N\"]\n",
    "    combo_result[\"max_wait_last_N\"] = metrics[\"max_wait_last_N\"]\n",
    "    results.append(combo_result)\n",
    "\n",
    "    combo_name = f\"combo_{i+1}_\" + \"_\".join(f\"{k}-{v}\" for k, v in combo.items())\n",
    "\n",
    "    save_episode_plots(metrics, combo_name=combo_name)\n",
    "\n",
    "    os.makedirs(\"Trained_Models\", exist_ok=True)\n",
    "    model_path = f\"Trained_Models/{combo_name}.pt\"\n",
    "    torch.save(metrics[\"trained_cnn_model\"].state_dict(), model_path)\n",
    "\n",
    "    num_scenarios = 4\n",
    "    scenario_rewards = [[] for _ in range(num_scenarios)]\n",
    "\n",
    "    for i, reward in enumerate(metrics[\"rewards_per_episode\"]):\n",
    "        scenario_index = i % num_scenarios\n",
    "        scenario_rewards[scenario_index].append(reward)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    for i, rewards in enumerate(scenario_rewards):\n",
    "        plt.plot(rewards, marker='o', label=f\"Scenario P{i+1}\")\n",
    "\n",
    "    plt.xlabel('Cycle (Episode Group)')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.title(f'Rewards per Scenario for combo: {combo}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c695b050",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(\"grid_search_results_cnn.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top 5 configs\n",
    "print(results_df.sort_values(\"avg_reward\", ascending=False).head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2b47a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_scenarios = 4\n",
    "\n",
    "# # Split the series into 4 separate lists\n",
    "# scenario_rewards = [[] for _ in range(num_scenarios)]\n",
    "# for i, reward in enumerate(rewards_per_episode):\n",
    "#     scenario_rewards[i % num_scenarios].append(reward)\n",
    "\n",
    "# # Plot each scenario as a separate line\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# for i in range(num_scenarios):\n",
    "#     plt.plot(scenario_rewards[i], marker='o', label=f'Scenario {i}')\n",
    "\n",
    "# plt.xlabel('Cycle')\n",
    "# plt.ylabel('Reward')\n",
    "# plt.title('Rewards per Scenario')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372b5321",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
